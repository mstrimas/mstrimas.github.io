<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spatial | Matt Strimas-Mackey</title>
    <link>/tags/spatial/</link>
      <atom:link href="/tags/spatial/index.xml" rel="self" type="application/rss+xml" />
    <description>Spatial</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Matt Strimas-Mackey 2020</copyright><lastBuildDate>Thu, 02 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Spatial</title>
      <link>/tags/spatial/</link>
    </image>
    
    <item>
      <title>Extracting eBird Data From a Polygon</title>
      <link>/post/extracting-ebird-data-polygon/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/extracting-ebird-data-polygon/</guid>
      <description>


&lt;p&gt;One of the first things I took on when I started at the Cornell Lab of Ornithology was creating the &lt;a href=&#34;https://cornelllabofornithology.github.io/auk/&#34;&gt;&lt;code&gt;auk&lt;/code&gt; R package&lt;/a&gt; for accessing eBird data. The entire eBird dataset can be downloaded as a massive text file, called the eBird Basic Dataset (EBD), and &lt;code&gt;auk&lt;/code&gt; pulls out manageable chunks of the dataset based on various spatial, temporal, or taxonomic filters. I’m often asked “how do I extract data from within a polygon?” (usually “polygon” is replaced by “shapefile”, but I try to avoid that word since there’s &lt;a href=&#34;http://switchfromshapefile.org/&#34;&gt;good reasons to stop using shapefiles&lt;/a&gt;). Rather than answer these questions individually, I thought I’d do a quick post about how to do this with &lt;code&gt;auk&lt;/code&gt;. Note that, at the time of posting, this requires some new &lt;code&gt;auk&lt;/code&gt; functionality that’s only in the development version of &lt;code&gt;auk&lt;/code&gt;, which can be installed with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(&amp;quot;CornellLabofOrnithology/auk&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details on &lt;code&gt;auk&lt;/code&gt; and eBird data in general, including how to get access to the EBD, it’s worth reading the first two chapters of the &lt;a href=&#34;https://cornelllabofornithology.github.io/ebird-best-practices/&#34;&gt;eBird Best Practices book&lt;/a&gt;. For the sake of speed and smaller file size, I’ll be working on a subset of the EBD containing all Northern Bobwhite records from 2019, which I obtained using the &lt;a href=&#34;https://cornelllabofornithology.github.io/ebird-best-practices/ebird.html#ebird-size-custom&#34;&gt;EBD custom download form&lt;/a&gt;, and you can &lt;a href=&#34;https://github.com/mstrimas/strimasdotcom/raw/master/content/post/2020-04-02-extracting-ebird-data-polygon/ebd_norbob_201901_201912_relFeb-2020.zip&#34;&gt;access here&lt;/a&gt;. However, everything I’ll show in this post works equally as well (just a lot slower!) on the full EBD. For this example, let’s say we want to extract all records from within a polygon defining &lt;a href=&#34;https://nabci-us.org/resources/bird-conservation-regions/&#34;&gt;Bird Conservation Region&lt;/a&gt; 27 (&lt;a href=&#34;https://nabci-us.org/resources/bird-conservation-regions-map/#bcr27&#34;&gt;Southeastern Coastal Plains&lt;/a&gt;). A GeoPackage of this region is available on the GitHub repository for the eBird Best Practices book, &lt;a href=&#34;https://github.com/CornellLabofOrnithology/ebird-best-practices/raw/master/data/gis-data.gpkg&#34;&gt;download it&lt;/a&gt;, place it in the &lt;code&gt;data/&lt;/code&gt; subdirectory of your RStudio project, then load it into R with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
library(auk)
library(dplyr)

poly &amp;lt;- read_sf(&amp;quot;data/gis-data.gpkg&amp;quot;, layer = &amp;quot;bcr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have a shapefile, replace &lt;code&gt;&#34;data/gis-data.gpkg&#34;&lt;/code&gt; with the path to your shapefile and omit &lt;code&gt;layer = &#34;bcr&#34;&lt;/code&gt;. Now that we have a polygon, extracting eBird data is a two step process:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Extract data from the EBD that’s within a bounding box containing the polygons using the function &lt;code&gt;auk_bbox()&lt;/code&gt;. This is necessary because due to the way &lt;code&gt;auk&lt;/code&gt; works under the hood, it can only filter to ranges of latitudes and longitudes.&lt;/li&gt;
&lt;li&gt;Import the resulting data into R and further subset it to just the observations that fall within the polygon.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fortunately, step 1 is made easier by &lt;code&gt;auk_bbox()&lt;/code&gt; accepting spatial &lt;code&gt;sf&lt;/code&gt; or &lt;code&gt;raster&lt;/code&gt; objects and automatically calculating the bounding box for you. For example,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auk_ebd(&amp;quot;data/ebd_norbob_201901_201912_relFeb-2020.txt&amp;quot;) %&amp;gt;% 
  auk_bbox(poly)
#&amp;gt; Input 
#&amp;gt;   EBD: /Users/mes335/projects/strimasdotcom/content/post/2020-04-02-extracting-ebird-data-polygon/data/ebd_norbob_201901_201912_relFeb-2020.txt 
#&amp;gt; 
#&amp;gt; Output 
#&amp;gt;   Filters not executed
#&amp;gt; 
#&amp;gt; Filters 
#&amp;gt;   Species: all
#&amp;gt;   Countries: all
#&amp;gt;   States: all
#&amp;gt;   BCRs: all
#&amp;gt;   Bounding box: Lon -91.6 - -75.5; Lat 29.3 - 37.3
#&amp;gt;   Date: all
#&amp;gt;   Start time: all
#&amp;gt;   Last edited date: all
#&amp;gt;   Protocol: all
#&amp;gt;   Project code: all
#&amp;gt;   Duration: all
#&amp;gt;   Distance travelled: all
#&amp;gt;   Records with breeding codes only: no
#&amp;gt;   Complete checklists only: no&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the output of the above command says &lt;code&gt;Bounding box: Lon -91.6 - -75.5; Lat 29.3 - 37.3&lt;/code&gt;, which are the bounds of the smallest square that contains the polygon. Let’s follow the method &lt;a href=&#34;https://cornelllabofornithology.github.io/ebird-best-practices/ebird.html#ebird-extract&#34;&gt;outlined in the Best Practices book&lt;/a&gt; to extract some data! We’ll get all observations on complete checklists from May to August inside the bounding box of the polygon:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_out &amp;lt;- &amp;quot;data/ebd_norbob_poly.txt&amp;quot;
auk_ebd(&amp;quot;data/ebd_norbob_201901_201912_relFeb-2020.txt&amp;quot;) %&amp;gt;% 
  # define filters
  auk_bbox(poly) %&amp;gt;% 
  auk_date(c(&amp;quot;*-05-01&amp;quot;, &amp;quot;*-08-31&amp;quot;)) %&amp;gt;% 
  auk_complete() %&amp;gt;% 
  # compile and run filters
  auk_filter(f_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results were output to a file, which you can read in with &lt;code&gt;read_ebd()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ebd &amp;lt;- read_ebd(&amp;quot;data/ebd_norbob_poly.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are now in a data frame and it’s time to proceed to step 2: further subset the data to only keep points within the polygon. First we’ll convert this data frame to a spatial &lt;code&gt;sf&lt;/code&gt; object using the &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;longitude&lt;/code&gt; columns, then well use &lt;code&gt;st_within()&lt;/code&gt; to identify the points within the polygon, and use this to subset the data frame. Note that we have to be careful with our coordinate reference system here: &lt;code&gt;crs = 4326&lt;/code&gt; specifies that the EBD data are in unprojected, lat-long coordinates and we use &lt;code&gt;st_transform()&lt;/code&gt; to ensure the polygons and points are in the coordinate reference system.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert to sf object
ebd_sf &amp;lt;- ebd %&amp;gt;% 
  select(longitude, latitude) %&amp;gt;% 
  st_as_sf( coords = c(&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;), crs = 4326)

# put polygons in same crs
poly_ll &amp;lt;- st_transform(poly, crs = st_crs(ebd_sf))

# identify points in polygon
in_poly &amp;lt;- st_within(ebd_sf, poly_ll, sparse = FALSE)

# subset data frame
ebd_in_poly &amp;lt;- ebd[in_poly[, 1], ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s create a simple map showing the EBD observations before (in black) and after (in green) subsetting the data to be within the polygon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 0, 0, 0))
plot(poly %&amp;gt;% st_geometry(), col = &amp;quot;grey40&amp;quot;, border = NA)
plot(ebd_sf, col = &amp;quot;black&amp;quot;, pch = 19, cex = 0.5, add = TRUE)
plot(ebd_sf[in_poly[, 1], ], 
     col = &amp;quot;forestgreen&amp;quot;, pch = 19, cex = 0.5, 
     add = TRUE)
legend(&amp;quot;top&amp;quot;, 
       legend = c(&amp;quot;All observations&amp;quot;, &amp;quot;After spatial subsetting&amp;quot;), 
       col = c(&amp;quot;grey40&amp;quot;, &amp;quot;forestgreen&amp;quot;), 
       pch = 19,
       bty = &amp;quot;n&amp;quot;,
       ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-02-extracting-ebird-data-polygon/index_files/figure-html/plot-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like it worked! We got just the points within the polygon as intended. Two final notes:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you’re working with the full EBD (a 200+ GB file), you’ll need to follow step 1 and subset the data using &lt;code&gt;auk&lt;/code&gt; prior to importing into R. However, if you’ve used the custom download form to get an EBD subset, your file is likely small enough that you can read the data directly into R with &lt;code&gt;read_ebd()&lt;/code&gt; and skip straight to step 2.&lt;/li&gt;
&lt;li&gt;If your intention is to eventually &lt;a href=&#34;https://cornelllabofornithology.github.io/ebird-best-practices/ebird.html#ebird-zf&#34;&gt;zero-fill the EBD&lt;/a&gt; to produce presence-absence data you’ll need to include the sampling event data file in the &lt;code&gt;auk_ebd()&lt;/code&gt;, subset both the EBD and sampling event data files separately to points within the polygon, the combine them together and zero-fill with &lt;code&gt;auk_zerofill()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Raster Summarization in Python</title>
      <link>/post/raster-summarization-in-python/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/raster-summarization-in-python/</guid>
      <description>


&lt;p&gt;As part of the &lt;a href=&#34;https://ebird.org/science/&#34;&gt;eBird Status &amp;amp; Trends&lt;/a&gt; team, I often find myself having to quickly summarize large rasters across layers. For examples, we might summarize weekly relative abundance layers for a single species across the weeks to estimate year round abundance or, for a given week, we might summarize across species to produce a &lt;a href=&#34;https://en.wikipedia.org/wiki/Species_richness&#34;&gt;species richness&lt;/a&gt; layer. I previously talked about how to perform these summarizations efficiently in R &lt;a href=&#34;/post/processing-large-rasters-in-r/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;/post/2020-03-24-block-processing-rasters/&#34;&gt;here&lt;/a&gt;. However, despite my best efforts, I could get the &lt;code&gt;raster&lt;/code&gt; R package to perform as quickly as I wanted it to. After a little googling, I started to think maybe the answer to “how do I quickly summarize rasters across layers in R?” might be “screw it, use Python!”.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://gdal.org/programs/gdal_calc.html&#34;&gt;&lt;code&gt;gdal_calc.py&lt;/code&gt;&lt;/a&gt; tool is a general purpose raster calculator using GDAL. You can do all sorts of stuff with it: adding layers, multiplying layers, reclassification, and calculating the mean or sum, among many other things. I did a few tests, and it seemed to be faster than &lt;code&gt;raster&lt;/code&gt; in R. There are three issues that made it a pain to work with though. First, you’re limited to working with a maximum of 26 layers or bands, and we often need to summarize across 52 weeks or hundreds of species. Second, in an effort to allow any possible type of raster algebra, the syntax is a bit verbose. For example, to find the cell-wise mean across two raster files, you’d use:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;gdal_calc.py -A input.tif -B input2.tif --outfile=result.tif --calc=&amp;quot;(A+B)/2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which becomes cumbersome when you have many rasters. Finally, I found it hard to get &lt;code&gt;gdal_calc.py&lt;/code&gt; to play nicely with missing values. I wanted to &lt;code&gt;gdal_calc.py&lt;/code&gt; to ignore missing values similarly to &lt;code&gt;mean(x, na.rm = TRUE)&lt;/code&gt; in R, but it wasn’t doing that.&lt;/p&gt;
&lt;p&gt;To overcome these issues, I decided to teach myself Python and create my own tool, aided by the source code of &lt;code&gt;gdal_calc.py&lt;/code&gt;, that’s specifically designed for summarizing raster data across layers or bands. This tool is &lt;code&gt;gdal-summarize.py&lt;/code&gt; and is &lt;a href=&#34;https://github.com/mstrimas/gdal-summarize&#34;&gt;available on GitHub&lt;/a&gt;. I hope others will use it and provide feedback; this is my first foray into Python!&lt;/p&gt;
&lt;div id=&#34;gdal_summarize.py&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;gdal_summarize.py&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The goal of &lt;code&gt;gdal-summarize.py&lt;/code&gt; is to summarize raster data across layers or bands. There are two common use cases for this tool. The first is calculating a cell-wise summary across the bands of a raster file (e.g. a GeoTIFF). For example, given a multi-band input GeoTIFF file &lt;code&gt;input.tif&lt;/code&gt;, to calculate the cell-wise sum of the first three bands use:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;gdal-summarize.py input.tif --bands 1 2 3 --outfile output.tif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, to compute the cell-wise sum across multiple GeoTIFF files (&lt;code&gt;input1.tif&lt;/code&gt;, &lt;code&gt;input2.tif&lt;/code&gt;, and &lt;code&gt;input3.tif&lt;/code&gt;) use:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;gdal-summarize.py input1.tif input2.tif input3.tif --outfile output.tif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If these input files have multiple bands, the default behavior is to summarize them across the &lt;strong&gt;first&lt;/strong&gt; band of each file; however, the &lt;code&gt;--bands&lt;/code&gt; argument can override this behavior:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# summarize across the second band of each file
gdal-summarize.py input1.tif input2.tif input3.tif --bands 2 --outfile output.tif
# summarize across band 1 of input1.tif and band 2 of input2.tif
gdal-summarize.py input1.tif input2.tif --bands 1 2 --outfile output.tif&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;summary-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary functions&lt;/h3&gt;
&lt;p&gt;The default behavior is to perform a cell-wise sum; however, other summary functions are available via the &lt;code&gt;--function&lt;/code&gt; argument:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt;: cell-wise sum across layers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mean&lt;/code&gt;: cell-wise mean across layers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;count&lt;/code&gt;: count the number layers with non-negative value for each cell.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;richness&lt;/code&gt;: count the number of layers with positive values for each cell.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all cases, these functions remove missing values (i.e. NoData or NA) prior to calculating the summary statistic. For example, the mean of the numners, 1, 2, and NoData is 1.5. This is similar to the way &lt;code&gt;na.rm = TRUE&lt;/code&gt; works in R. I’d be happy to add additional summary functions, just &lt;a href=&#34;https://github.com/mstrimas/gdal-summarize/issues/new/choose&#34;&gt;open an issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/mstrimas/gdal-summarize/compare&#34;&gt;submit a pull request&lt;/a&gt; on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;block-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Block processing&lt;/h3&gt;
&lt;p&gt;To avoid having to read an entire raster file into memory, &lt;code&gt;gdal-summarize.py&lt;/code&gt; processes input rasters in blocks; it will read in a block of data, summarize it, then output to file, before proceeding to the next block. GDAL will suggest a “natural” block size for efficiently processing the data (typically 256X256 cells for GeoTIFFs), but users can also specify their own block size with command line arguments in one of two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--block_size/-s&lt;/code&gt;: two integers given the x and y dimensions of the block. Note that the x dimension corresponds to columns, while the y dimension corresponds to rows.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--nrows/-n&lt;/code&gt;: an integer specifying the number of rows in each block. If this approach is used, the raster will be processed in groups of entire rows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s not clear to me what the most efficient block size is, so using the default block size is probably a good starting point. This &lt;a href=&#34;https://gis.stackexchange.com/questions/172666/optimizing-python-gdal-readasarray&#34;&gt;StackExchange question&lt;/a&gt; has some thoughts on block size and efficiency and suggests it could be worth doing some tests to see what works best for your scenario.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-python-and-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing Python and R&lt;/h2&gt;
&lt;p&gt;Everyone loves a good Python vs. R battle, so let’s put &lt;code&gt;calc()&lt;/code&gt; from the &lt;code&gt;raster&lt;/code&gt; R package up against &lt;code&gt;gdal-summarize.py&lt;/code&gt;. I use the same &lt;a href=&#34;https://ebird.org/science/status-and-trends/woothr&#34;&gt;Wood Thrush&lt;/a&gt; relative abundance raster from eBird Status and Trends as I used in my &lt;a href=&#34;/post/2020-03-24-block-processing-rasters/&#34;&gt;previous post&lt;/a&gt;, consisting of 16 bands with dimensions 5630X7074. Since &lt;code&gt;raster&lt;/code&gt; processes data in blocks of rows, I’ll do the same thing for &lt;code&gt;gdal-summarize.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(raster)
library(tidyverse)

f &amp;lt;- &amp;quot;data/woothr.tif&amp;quot;
r &amp;lt;- stack(f)

# choose the values of n_rows to try
n_rows &amp;lt;- 2^seq(0, ceiling(log2(nrow(r))))
n_rows[length(n_rows)] &amp;lt;- nrow(r)

# set up 10 repetitions of each n_rows value
nrow_grid &amp;lt;- expand_grid(rep = 1:10, n_rows = n_rows)

# summarize raster using each value of n_rows
summarize_nrows &amp;lt;- function(x) {
  # time r
  t_r &amp;lt;- system.time({
    r_mean &amp;lt;- summarize_raster(r, &amp;quot;mean&amp;quot;, n_rows = x)
  })
  # time python
  f_out &amp;lt;- &amp;quot;woothr-mean.tif&amp;quot;
  t_py &amp;lt;- system.time({
    str_glue(&amp;quot;source ~/.bash_profile;&amp;quot;,
             &amp;quot;gdal-summarize.py {f} -o {f_out} -w -f &amp;#39;mean&amp;#39; -n {x}&amp;quot;) %&amp;gt;% 
      system()
  })
  tibble(elapsed_r = t_r[&amp;quot;elapsed&amp;quot;], elapsed_py = t_py[&amp;quot;elapsed&amp;quot;])
}
time_summarize &amp;lt;- nrow_grid %&amp;gt;% 
  mutate(results = map(n_rows, summarize_nrows)) %&amp;gt;% 
  unnest(cols = results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take look at how they compare:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# x-axis breaks
brks &amp;lt;- 2^seq(0, ceiling(log2(nrow(r))), by = 2)
brks[length(brks)] &amp;lt;- nrow(r)

# transform to longer for ggplot
time_summarize &amp;lt;- time_summarize %&amp;gt;% 
  pivot_longer(cols = c(elapsed_r, elapsed_py)) %&amp;gt;% 
  mutate(name = recode(name, &amp;quot;elapsed_r&amp;quot; = &amp;quot;R&amp;quot;, &amp;quot;elapsed_py&amp;quot; = &amp;quot;Python&amp;quot;))

# plot
ggplot(time_summarize) +
  aes(x = n_rows, y = value / 60, group = name, color = name) +
  geom_point(alpha = 0.25, size = 0.75) +
  geom_smooth(method = &amp;quot;gam&amp;quot;) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;, breaks = brks) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;) +
  labs(x = &amp;quot;# of rows/block size (log2 scale)&amp;quot;, 
       y = &amp;quot;Elapsed time (minutes)&amp;quot;,
       color = NULL,
       title = &amp;quot;Python vs. R raster summarization&amp;quot;,
       subtitle = &amp;quot;5630x7074 raster, mean of 16 layers&amp;quot;) +
  theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-26-raster-summarization-in-python/index_files/figure-html/time-plot-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, not only is Python raster than R, but Python experiences much less of a block size effect.&lt;/p&gt;
&lt;p&gt;Hopefully others will find this tool useful! Please take a look through the &lt;a href=&#34;https://github.com/mstrimas/gdal-summarize&#34;&gt;GitHub repo&lt;/a&gt; and let me know if there’s something I could be doing differently. As I said at the start, this is my first foray into Python, so I’m bound to have made some mistakes.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Block Processing of Rasters in R</title>
      <link>/post/block-processing-rasters/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/block-processing-rasters/</guid>
      <description>


&lt;p&gt;In &lt;a href=&#34;/post/processing-large-rasters-in-r/&#34;&gt;my previous post&lt;/a&gt;, I tried to understand how &lt;code&gt;raster&lt;/code&gt; package functions like &lt;code&gt;calc()&lt;/code&gt; process raster data. The package does a good job of abstracting a lot of this detail away from users, data will be processed either on disk or in memory based on criteria related to the size of the raster, the available resources, and user-defined parameters. In addition, if processing is done on disk, the raster will be processed in blocks, the size of which is user controlled. In most cases, &lt;code&gt;raster&lt;/code&gt; makes sensible choices and the whole process works seamlessly for the user. However, I’ve found that, for large raster datasets, the specific details of &lt;em&gt;how&lt;/em&gt; a raster is processed in blocks can make a big difference in terms of computational efficient. Furthermore, with &lt;code&gt;raster&lt;/code&gt; it’s hard to know exactly what’s going on behind the scenes and the parameters that we can tune through &lt;code&gt;rasterOptions()&lt;/code&gt; don’t give me as much direct control over the processing as I’d like. In this post, I’m going to try taking lower level control of &lt;code&gt;raster&lt;/code&gt; processing and explore how choices about block size impact processing speed.&lt;/p&gt;
&lt;p&gt;To demonstrate all this, I’m going to use some real data from the &lt;a href=&#34;https://ebird.org/science/status-and-trends/woothr/abundance-map&#34;&gt;eBird Status &amp;amp; Trends&lt;/a&gt;; this is a dataset that I work with on a daily basis in my job. The below code downloads weekly relative abundance for Wood Thrush at 3 km resolution across the entire western hemisphere, subsets the annual data to just the weeks of the breeding season (May 24 - September 7), then saves the results as a GeoTIFF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ebirdst)
library(raster)
library(tictoc)
library(profmem)
library(tidyverse)
# download status and trends data
abd &amp;lt;- get_species_path(&amp;quot;woothr&amp;quot;) %&amp;gt;% 
  load_raster(&amp;quot;abundance&amp;quot;, .)
# subset to the breeding season
dts &amp;lt;- parse_raster_dates(abd)
r &amp;lt;- abd[[which(dts &amp;gt;= as.Date(&amp;quot;2018-05-24&amp;quot;) &amp;amp; dts &amp;lt;= as.Date(&amp;quot;2018-09-07&amp;quot;))]]
r &amp;lt;- writeRaster(r, &amp;quot;data/woothr.tif&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a big dataset, almost 40 million cells and 16 layers, which works out to be just under 5 GB when read into memory, assuming 8 bytes per cell.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(r)
#&amp;gt; class      : RasterStack 
#&amp;gt; dimensions : 5630, 7074, 39826620, 16  (nrow, ncol, ncell, nlayers)
#&amp;gt; resolution : 2963, 2963  (x, y)
#&amp;gt; extent     : -2e+07, 943785, -6673060, 1e+07  (xmin, xmax, ymin, ymax)
#&amp;gt; crs        : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs 
#&amp;gt; names      : woothr.1, woothr.2, woothr.3, woothr.4, woothr.5, woothr.6, woothr.7, woothr.8, woothr.9, woothr.10, woothr.11, woothr.12, woothr.13, woothr.14, woothr.15, ... 
#&amp;gt; min values :        0,        0,        0,        0,        0,        0,        0,        0,        0,         0,         0,         0,         0,         0,         0, ... 
#&amp;gt; max values :     4.86,     4.76,     4.59,     4.94,     5.68,     5.59,     5.32,     4.98,     4.62,      3.75,      2.84,      2.30,      2.36,      2.57,      2.93, ...
# estimate size in gb
8 * ncell(r) * nlayers(r) / 2^30
#&amp;gt; [1] 4.75&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What I want to do with this dataset, is simply average across the weeks to produce a single layer of breeding season abundance, which is what we use to generate the seasonal maps on the &lt;a href=&#34;https://ebird.org/science/status-and-trends/woothr/abundance-map-breeding&#34;&gt;Status and Trends website&lt;/a&gt;. As I showed in the previous post, that can be done with &lt;code&gt;calc()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_mean &amp;lt;- calc(r, mean, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This took about 2 minutes to run on my laptop and, given the size, it must have been processed on disk in blocks rather than in memory. Recall that the block size is the number of rows that are read in and processed at a time. The catch is that it’s not clear what block size was used and if the automatically chosen block size was optimal. You can use &lt;code&gt;rasterOptions()&lt;/code&gt; to set &lt;code&gt;chunksize&lt;/code&gt; in bytes, which indirectly controls the number of rows per block. Throughout this post, I’ll use “block size” to refer to the number of rows per block and “chunk size” to refer to the user-defined maximum number of bytes available for each block. If you consult the &lt;a href=&#34;https://github.com/rspatial/raster/blob/master/R/calc.R&#34;&gt;source code for &lt;code&gt;calc()&lt;/code&gt;&lt;/a&gt; it’s possible to determine that this function splits rasters into blocks using &lt;code&gt;blockSize()&lt;/code&gt; with &lt;code&gt;n = 2 * (nlayers(r) + 1)&lt;/code&gt;, then you’d have to look at the &lt;a href=&#34;https://github.com/rspatial/raster/blob/master/R/blockSize.R&#34;&gt;source code for &lt;code&gt;blockSize()&lt;/code&gt;&lt;/a&gt; to determine that this is converted to the number of rows per blocks with &lt;code&gt;floor(chunksize / (8 * n * ncol(r)))&lt;/code&gt;. So, working through all this with the default chunk size used by raster (&lt;span class=&#34;math inline&#34;&gt;\(10^8\)&lt;/span&gt;), we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 2 * (nlayers(r) + 1)
1e8 / (8 * n * ncol(r))
#&amp;gt; [1] 52&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In most cases, you’re probably best to just set &lt;code&gt;chunksize&lt;/code&gt; based on your system’s resources and leave the block size choice to &lt;code&gt;raster&lt;/code&gt;, but for sake of optimizing the processing of the Status &amp;amp; Trends data, I want to know exactly what block size is being used and, ideally, be able to control the block size directly rather than via the chunk size. With this is mind, I forked the &lt;a href=&#34;https://github.com/rspatial/raster/&#34;&gt;&lt;code&gt;raster&lt;/code&gt; GitHub repo&lt;/a&gt; and made some modifications.&lt;/p&gt;
&lt;div id=&#34;modifying-the-raster-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modifying the &lt;code&gt;raster&lt;/code&gt; package&lt;/h2&gt;
&lt;p&gt;After &lt;a href=&#34;https://github.com/mstrimas/raster&#34;&gt;forking the &lt;code&gt;raster&lt;/code&gt; package repository&lt;/a&gt; I made the following changes:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;calc()&lt;/code&gt; now displays the block size (i.e. number of rows) being used.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;calc(return_blocks = TRUE)&lt;/code&gt; will, instead of processing the raster object, just return the blocks that would have been used.&lt;/li&gt;
&lt;li&gt;Added a new function, &lt;code&gt;summarize_raster()&lt;/code&gt;, that’s a simplified version of &lt;code&gt;calc()&lt;/code&gt;. It always process files on disk, will only calculate means and sums rather than a generic input function, always uses &lt;code&gt;na.rm = TRUE&lt;/code&gt;, and it takes either a &lt;code&gt;chunksize&lt;/code&gt; argument (in units of available GB of RAM) or an &lt;code&gt;n_rows&lt;/code&gt; argument that directly controls the number of rows per block.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you’re interested, the source code for the &lt;code&gt;summarize_raster()&lt;/code&gt; function is &lt;a href=&#34;https://github.com/mstrimas/raster/blob/master/R/summarize-raster.R&#34;&gt;here&lt;/a&gt;. It removes much of the functionality of &lt;code&gt;calc()&lt;/code&gt; but this comes with the benefit of being easier to figure out what’s going on inside the function. If you want to install this version of the &lt;code&gt;raster&lt;/code&gt; package you could do so with &lt;code&gt;remotes::install_github(&#34;mstrimas/raster&#34;)&lt;/code&gt;, but beware I’ve modified the original package in a very sloppy way and &lt;strong&gt;you probably don’t want to overwrite the real &lt;code&gt;raster&lt;/code&gt; package with my hacked version.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-what-block-size-is-being-used&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;So, what block size &lt;em&gt;is&lt;/em&gt; being used?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic()
r_mean_calc &amp;lt;- calc(r, mean, na.rm = TRUE)
#&amp;gt; Using 111 blocks of 51 rows.
toc()
#&amp;gt; 127.619 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, with the default chunk size of &lt;span class=&#34;math inline&#34;&gt;\(10^8 \approx 0.1 GB\)&lt;/span&gt;, &lt;code&gt;calc()&lt;/code&gt; processes this raster in 111 blocks of 51 rows each. We can use my custom &lt;code&gt;summarize_raster()&lt;/code&gt; function to do the same thing, but specify block size explicitly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic()
r_mean_summ &amp;lt;- summarize_raster(r, &amp;quot;mean&amp;quot;, n_rows = 51)
#&amp;gt; Using 111 blocks of 51 rows.
toc()
#&amp;gt; 125.852 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out &lt;code&gt;summarize_raster()&lt;/code&gt; is slightly faster because I’ve slimmed it down and removed some additional checks and functionality. We can confirm that the two approaches produce the same results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cellStats(abs(r_mean_calc - r_mean_summ), max)
#&amp;gt; [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;run-time-vs.-block-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Run time vs. block size&lt;/h2&gt;
&lt;p&gt;Ok, now everything’s in place, and we can examine how block size impacts processing time. Recall that block size is the number of rows that get read in, summarized, and output at each iteration of the processing. Let’s estimate run time for a range of blocks sizes, from 1 row to 5,630 rows, equivalent to processing the whole raster in memory. Note that I’m running this on my laptop, which has 16 GB of RAM, so I have more than enough resources to process this raster in memory, but I’m choosing to process it on disk for demonstration purposes. Since there could be variation in run time, I’ll summarize the raster 10 times for each block size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# choose the values of n_rows to try
n_rows &amp;lt;- 2^seq(0, ceiling(log2(nrow(r))))
n_rows[length(n_rows)] &amp;lt;- nrow(r)

# set up 10 repetitions of each n_rows value
nrow_grid &amp;lt;- expand_grid(rep = 1:10, n_rows = n_rows)

# summarize raster using each value of n_rows
summarize_nrows &amp;lt;- function(x) {
  # memory usage
  m &amp;lt;- profmem({
    # timer
    t &amp;lt;- system.time({
      r_mean &amp;lt;- summarize_raster(r, &amp;quot;mean&amp;quot;, n_rows = x)
    })
  })
  return(tibble(elapsed = t[&amp;quot;elapsed&amp;quot;], max_mem = max(m$bytes, na.rm = TRUE)))
}
time_mem &amp;lt;- nrow_grid %&amp;gt;% 
  mutate(results = map(n_rows, summarize_nrows)) %&amp;gt;% 
  unnest(cols = results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s plot the elapsed time vs. block size curve. I’ll show both the raw data, consisting of 10 for each block size, and a smoothed function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(time_mem) +
  aes(x = n_rows, y = elapsed / 60) +
  geom_point(alpha = 0.25, size = 0.75) +
  geom_smooth(method = &amp;quot;gam&amp;quot;) +
  labs(x = &amp;quot;# of rows/block size (log2 scale)&amp;quot;, 
       y = &amp;quot;Elapsed time (minutes)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-24-block-processing-rasters/index_files/figure-html/vs-plot-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting, it looks like there’s an exponential decrease in processing time as block size increases. This suggests a log-transformed x-axis would be worthwhile. I’m also going to show the default block size explicitly on the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(time_mem) +
  aes(x = n_rows, y = elapsed / 60) +
  geom_point(alpha = 0.25, size = 0.75) +
  geom_smooth(method = &amp;quot;gam&amp;quot;) +
  geom_vline(xintercept = 51,
             color = &amp;quot;orange&amp;quot;,
             size = 1.2,
             linetype = &amp;quot;dashed&amp;quot;) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;) +
  labs(x = &amp;quot;# of rows/block size (log2 scale)&amp;quot;, 
       y = &amp;quot;Elapsed time (minutes)&amp;quot;,
       title = &amp;quot;Processing time decreases exponentially with block size&amp;quot;,
       subtitle = &amp;quot;Default block size (51 rows) shown in oranges&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-24-block-processing-rasters/index_files/figure-html/vs-logplot-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the decrease is exponential. The default block size using &lt;code&gt;chunksize = 1e8&lt;/code&gt; does a pretty good job, but it’s not giving us the most efficient processing given the memory resources available. This shouldn’t be too surprising since &lt;span class=&#34;math inline&#34;&gt;\(10^8\)&lt;/span&gt; bytes is quite small, so would should really be picking something more appropriate for our system’s available RAM. All this said, it’s worth noting these are fairly modest efficiency gains, but they can add up if you’re dealing with a large number of big raster files.&lt;/p&gt;
&lt;p&gt;I also estimated the maximum memory usage during the summarization, so let’s take a look at that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(time_mem) +
  aes(x = n_rows, y = max_mem / 2^30) +
  geom_line() +
  geom_point() +
  labs(x = &amp;quot;# of rows&amp;quot;, y = &amp;quot;Maximum memory used (GB)&amp;quot;,
       title = &amp;quot;Memory usage increases linearly with block size&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-24-block-processing-rasters/index_files/figure-html/vs-mem-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is as expected: large blocks mean more data is being processed at once, which leads to higher memory usage.&lt;/p&gt;
&lt;p&gt;The takeaway from all this is so obvious it’s almost embarrassing: if you want to process large rasters efficiently, increase the &lt;code&gt;chunksize&lt;/code&gt; option so the &lt;code&gt;raster&lt;/code&gt; package will process files in larger blocks and take advantage of the RAM you have available. Despite it being obvious, I suspect most people are like me and blindly go with the default raster options, which is fine in many cases, but becomes problematic for large rasters.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Processing Large Rasters in R</title>
      <link>/post/processing-large-rasters-in-r/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/processing-large-rasters-in-r/</guid>
      <description>


&lt;p&gt;We work with a lot of large raster datasets on the &lt;a href=&#34;https://ebird.org/science/&#34;&gt;eBird Status &amp;amp; Trends&lt;/a&gt; project, and processing them is becoming a real bottleneck in our R workflow. For example, we make weekly estimates of bird abundance at 3 km resolution across the entire Western Hemisphere, which results in raster stacks with billions of cells! To produce seasonal abundance maps, we need to average the weekly layers across all weeks within each season using the &lt;code&gt;raster&lt;/code&gt; function &lt;code&gt;calc()&lt;/code&gt;, and it takes forever with these huge files! In this post, I’m going to try to understand how &lt;code&gt;raster&lt;/code&gt; processes data and explore how this can be tweaked to improve computational efficiency. Most of the material is covered in greater detail in &lt;a href=&#34;https://rspatial.org/raster/RasterPackage.pdf&#34;&gt;the &lt;code&gt;raster&lt;/code&gt; package vignette&lt;/a&gt;, especially Chapter 10 of that document.&lt;/p&gt;
&lt;p&gt;In general, R holds objects in memory, which results in a limit to the size of objects that can be processed. This poses a problem for processing raster datasets, which can be much larger than the available system memory. The &lt;code&gt;raster&lt;/code&gt; package addresses this by only storing references to raster files within its &lt;code&gt;Raster*&lt;/code&gt; objects. Depending on the memory requirements for a given raster calculation, and the memory available, the package functions will either read the whole dataset into R for processing or process it in smaller chunks.&lt;/p&gt;
&lt;p&gt;Let’s start by importing an example dataset generated using the &lt;code&gt;simulate_species()&lt;/code&gt; function from the &lt;code&gt;prioritizr&lt;/code&gt; package. The raster has dimensions 1000x1000 and 9 layers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(raster)
library(rasterVis)
library(viridis)
library(profmem)
library(tidyverse)

r &amp;lt;- stack(&amp;quot;data/large-raster.tif&amp;quot;)
print(r)
#&amp;gt; class      : RasterStack 
#&amp;gt; dimensions : 1000, 1000, 1e+06, 9  (nrow, ncol, ncell, nlayers)
#&amp;gt; resolution : 0.001, 0.001  (x, y)
#&amp;gt; extent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)
#&amp;gt; crs        : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 
#&amp;gt; names      : large.raster.1, large.raster.2, large.raster.3, large.raster.4, large.raster.5, large.raster.6, large.raster.7, large.raster.8, large.raster.9 
#&amp;gt; min values :              0,              0,              0,              0,              0,              0,              0,              0,              0 
#&amp;gt; max values :          0.885,          0.860,          0.583,          0.744,          0.769,          0.428,          0.289,          0.579,          0.499
levelplot(r,
          col.regions = viridis,
          xlab = NULL, ylab = NULL,
          scales = list(draw = FALSE),
          names.attr = paste(&amp;quot;Band&amp;quot;, seq_len(nlayers(r))),
          maxpixels = 1e6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-20-processing-large-rasters-in-r/index_files/figure-html/read-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can calculate the total number of values this raster can store and the associated memory requirements assuming 8 bytes per cell value. To calculate the actual memory used, we can override the default &lt;code&gt;raster&lt;/code&gt; behavior and read the contents of the file into R using &lt;code&gt;readAll()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_values &amp;lt;- ncell(r) * nlayers(r)
# memory in mb
mem_est &amp;lt;- 8 * n_values / 2^20
mem_act &amp;lt;- as.integer(object.size(readAll(r))) / 2^20
#&amp;gt; [1] &amp;quot;# values in raster:  9,000,000&amp;quot;
#&amp;gt; [1] &amp;quot;Estimated size (MB):  68.7&amp;quot;
#&amp;gt; [1] &amp;quot;Memory usage (MB):  68.8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we were fairly close in our estimates, looks like it takes a little under 70 Mb of memory to hold this object in the R session.&lt;/p&gt;
&lt;div id=&#34;processing-rasters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Processing rasters&lt;/h2&gt;
&lt;p&gt;Let’s apply the &lt;code&gt;calc()&lt;/code&gt; function to this dataset to calculate the cell-wise mean across layers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_mean &amp;lt;- calc(r, mean, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is essentially to equivalent of &lt;code&gt;apply()&lt;/code&gt; for a array with 3 dimensions, e.g.:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- array(runif(27), dim = c(3, 3, 3))
apply(a, 1:2, mean)
#&amp;gt;       [,1]  [,2]  [,3]
#&amp;gt; [1,] 0.236 0.602 0.570
#&amp;gt; [2,] 0.452 0.412 0.588
#&amp;gt; [3,] 0.561 0.598 0.545&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what &lt;code&gt;raster&lt;/code&gt; is doing in &lt;code&gt;calc()&lt;/code&gt; is a little different and depends on the memory requirements of the calculation. We can use the function &lt;code&gt;canProccessInMemory()&lt;/code&gt; to test whether a &lt;code&gt;Raster*&lt;/code&gt; object can be loaded into memory for processing or not. We’ll use &lt;code&gt;verbose = TRUE&lt;/code&gt; to get some additional information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;canProcessInMemory(r, verbose = TRUE)
#&amp;gt; memory stats in GB
#&amp;gt; mem available: 5.11
#&amp;gt;         60%  : 3.06
#&amp;gt; mem needed   : 0.27
#&amp;gt; max allowed  : 4.66  (if available)
#&amp;gt; [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells me how much free memory I have available on my computer and how much memory is required for this &lt;code&gt;Raster*&lt;/code&gt; object. We don’t want &lt;code&gt;raster&lt;/code&gt; eating up all our memory, so &lt;code&gt;raster&lt;/code&gt; has two user adjustable options to specify the maximum amount of memory it will use in bytes or relative to the available memory. These values default to 5 billion bytes (4.66 GB) and 60%, respectively, but can be adjusted with &lt;code&gt;rasterOptions()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;raster&lt;/code&gt; functions call &lt;code&gt;canProccessInMemory()&lt;/code&gt; when they’re invoked, then use a different approach for processing depending on the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;canProcessInMemory(r) == TRUE&lt;/code&gt;: read the entire object into the R session, then process all at once similar to &lt;code&gt;apply()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;canProcessInMemory(r) == FALSE&lt;/code&gt;: process the raster in blocks of rows, each of which is small enough to store in memory. This approach requires that the output raster object is saved in a file. Blocks of rows are read from the input files, processed in R, then written to the output file, and this is done iteratively for all the blocks until the whole raster is processed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One wrinkle to this is that each &lt;code&gt;raster&lt;/code&gt; function has different memory requirements. This is dealt with using the &lt;code&gt;n&lt;/code&gt; argument to &lt;code&gt;canProccessInMemory()&lt;/code&gt;, which specifies the number of copies of the &lt;code&gt;Raster*&lt;/code&gt; object’s cell values that the function needs to have in memory. Specifically, the estimated memory requirement in bytes is &lt;code&gt;8 * n * ncell(r) * nlayers(r)&lt;/code&gt;. Let’s see how different values of &lt;code&gt;n&lt;/code&gt; affect whether a raster can be processed in memory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(n = c(1, 10, 20, 30, 40, 50, 60)) %&amp;gt;% 
  mutate(process_in_mem = map_lgl(n, canProcessInMemory, x = r))
#&amp;gt; # A tibble: 7 x 2
#&amp;gt;       n process_in_mem
#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;         
#&amp;gt; 1     1 TRUE          
#&amp;gt; 2    10 TRUE          
#&amp;gt; 3    20 TRUE          
#&amp;gt; 4    30 TRUE          
#&amp;gt; 5    40 TRUE          
#&amp;gt; 6    50 FALSE         
#&amp;gt; 7    60 FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, even though I called this a “large” raster, R can still handle processing it in memory until we get to requiring a fairly large number of copies, at which time, the raster will switch to being processed in blocks. For reason I don’t fully understand, the &lt;a href=&#34;https://github.com/rspatial/raster/blob/master/R/calc.R&#34;&gt;source code of the &lt;code&gt;calc()&lt;/code&gt; function&lt;/a&gt; suggests that it’s using &lt;code&gt;n = 2 * (nlayers(r) + 1)&lt;/code&gt;, which is 20, so &lt;code&gt;calc()&lt;/code&gt; is processing this raster in memory on my system. Indeed, we can confirm that the result of this calculation are stored in a memory with &lt;code&gt;inMemory()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inMemory(r_mean)
#&amp;gt; [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What’s the point of going to all this trouble? If a raster can be processed in blocks to reduce memory usage, why not do it all the time? The issue is that processing in memory is much faster than processing in blocks and having to write to a file. We can see this by forcing &lt;code&gt;calc()&lt;/code&gt; to process on disk in blocks by setting &lt;code&gt;rasterOptions(todisk = TRUE)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in memory
rasterOptions(todisk = FALSE)
t_inmem &amp;lt;- system.time(calc(r, mean, na.rm = TRUE))
print(t_inmem)
#&amp;gt;    user  system elapsed 
#&amp;gt;   5.533   0.149   5.699

# on disk
rasterOptions(todisk = TRUE)
t_ondisk &amp;lt;- system.time(calc(r, mean, na.rm = TRUE))
print(t_ondisk)
#&amp;gt;    user  system elapsed 
#&amp;gt;   5.683   0.392   6.091
rasterOptions(todisk = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, we see a 6.9% increase in efficiency by processing in memory. The &lt;code&gt;profmem&lt;/code&gt; package can gives us some information on the different amounts of memory used for the two approaches. Specifically, we can estimate the maximum amount of memory used at any one time by &lt;code&gt;calc()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in memory
rasterOptions(todisk = FALSE)
m_inmem &amp;lt;- max(profmem(calc(r, mean, na.rm = TRUE))$bytes, na.rm = TRUE)

# on disk
rasterOptions(todisk = TRUE)
m_ondisk &amp;lt;- max(profmem(calc(r, mean, na.rm = TRUE))$bytes, na.rm = TRUE)
rasterOptions(todisk = FALSE)
#&amp;gt; [1] &amp;quot;In memory (MB):  69&amp;quot;
#&amp;gt; [1] &amp;quot;On disk (MB):  17&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, it’s clear that different ways of processing &lt;code&gt;Raster*&lt;/code&gt; objects affects both the processing time and resource use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;raster-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;raster&lt;/code&gt; options&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;raster&lt;/code&gt; package has a few options that can adjusted to tweak how functions process data. Let’s take a look at the default values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rasterOptions()
#&amp;gt; format        : raster 
#&amp;gt; datatype      : FLT4S 
#&amp;gt; overwrite     : FALSE 
#&amp;gt; progress      : none 
#&amp;gt; timer         : FALSE 
#&amp;gt; chunksize     : 1e+08 
#&amp;gt; maxmemory     : 5e+09 
#&amp;gt; memfrac       : 0.6 
#&amp;gt; tmpdir        : /var/folders/mg/qh40qmqd7376xn8qxd6hm5lwjyy0h2/T//Rtmp3inPgu/raster// 
#&amp;gt; tmptime       : 168 
#&amp;gt; setfileext    : TRUE 
#&amp;gt; tolerance     : 0.1 
#&amp;gt; standardnames : TRUE 
#&amp;gt; warn depracat.: TRUE 
#&amp;gt; header        : none&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The options relevant to memory and processing are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;maxmemory&lt;/code&gt;: the maximum amount of memory (in bytes) to use for a given operation, defaults to 5 billion bytes (4.66 GB).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;memfrac&lt;/code&gt;: the maximum proportion of the available memory to use for a given operation, defaults to 60%.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chunksize&lt;/code&gt;: the maximum size (in bytes) of individual chunks of data to read/write when a raster is being processed in blocks, defaults to 100 million bytes (0.1 GB).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;todisk&lt;/code&gt;: used to force processing on disk in blocks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, we can adjust &lt;code&gt;chunksize&lt;/code&gt; to force &lt;code&gt;calc()&lt;/code&gt; to process our raster stack in smaller pieces. Note that &lt;code&gt;raster&lt;/code&gt; actually ignores user specified values of &lt;code&gt;chunksize&lt;/code&gt; if they’re below &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt;, so I’ll have to do something sketchy and overwrite an internal &lt;code&gt;raster&lt;/code&gt; function to allow this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hack raster internal function
cs_orig &amp;lt;- raster:::.chunk
cs_hack &amp;lt;- function(x) getOption(&amp;quot;rasterChunkSize&amp;quot;)
assignInNamespace(&amp;quot;.chunk&amp;quot;, cs_hack, ns = &amp;quot;raster&amp;quot;)

# use 1 kb chunks
rasterOptions(chunksize = 1000, todisk = TRUE)
t_smallchunks &amp;lt;- system.time(calc(r, mean, na.rm = TRUE))

# undo the hack
assignInNamespace(&amp;quot;.chunk&amp;quot;, cs_orig, ns = &amp;quot;raster&amp;quot;)
rasterOptions(default = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Processing in smaller chunks resulted in a 56.4% decrease in efficiency compared to the default chunk size. All this suggests to me that, when dealing with large rasters, it makes sense to increase &lt;code&gt;maxmemory&lt;/code&gt; as much as feasible given the memory available on your system; the default value of ~ 1 GB is quite small for a modern computer. Then, once you get to a point where the raster has to be processed in blocks, increase &lt;code&gt;chunksize&lt;/code&gt; to take advantage of as much memory as you have available.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;processing-in-blocks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Processing in blocks&lt;/h2&gt;
&lt;p&gt;I want to take a quick detour to understand exactly how &lt;code&gt;raster&lt;/code&gt; processes data in blocks. Looking at the &lt;a href=&#34;https://github.com/rspatial/raster/blob/master/R/calc.R&#34;&gt;source code of the &lt;code&gt;calc()&lt;/code&gt; function&lt;/a&gt; gives a template for how this is done. A few &lt;code&gt;raster&lt;/code&gt; functions help with this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;blockSize()&lt;/code&gt; suggests a sensible way to break up a &lt;code&gt;Raster*&lt;/code&gt; object for processing in blocks. The &lt;code&gt;raster&lt;/code&gt; objects always uses a set of entire rows as blocks, so this function gives the starting row numbers of each block.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;readStart()&lt;/code&gt; opens a file on disk for reading.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getValues()&lt;/code&gt; reads a block of data, defined by the starting row and number of rows.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;readStop()&lt;/code&gt; closes the input file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;writeStart()&lt;/code&gt; opens a file on disk for writing the results of our calculations to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;writeValues()&lt;/code&gt; writes a block of data to a file, starting at a given row.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;writeStop()&lt;/code&gt; closes the output file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s set things up to replicate what &lt;code&gt;calc()&lt;/code&gt; does. First we need to determine how to dive the input raster up into blocks for processing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# file paths
f_in &amp;lt;- &amp;quot;data/large-raster.tif&amp;quot;
f_out &amp;lt;- tempfile(fileext = &amp;quot;.tif&amp;quot;)

# input and output rasters
r_in &amp;lt;- stack(f_in)
r_out &amp;lt;- raster(r_in)

# blocks
b &amp;lt;- blockSize(r_in)
print(b)
#&amp;gt; $row
#&amp;gt; [1]   1 251 501 751
#&amp;gt; 
#&amp;gt; $nrows
#&amp;gt; [1] 250 250 250 250
#&amp;gt; 
#&amp;gt; $n
#&amp;gt; [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, &lt;code&gt;blockSize()&lt;/code&gt; is suggesting we break the file up into 4 blocks (&lt;code&gt;b$n&lt;/code&gt;) of 250 (&lt;code&gt;b$nrows&lt;/code&gt;) each, and &lt;code&gt;b$rows&lt;/code&gt; gives us the starting row value for each block. Now we open the input and output files, process the blocks iteratively, reading and writing as necessary, then close the files.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# open files
r_in &amp;lt;- readStart(r_in)
r_out &amp;lt;- writeStart(r_out, filename = f_out)

# loop over blocks
for (i in seq_along(b$row)) {
  # read values for block
  # format is a matrix with rows the cells values and columns the layers
  v &amp;lt;- getValues(r_in, row = b$row[i], nrows = b$nrows[i])
  
  # mean cell value across layers
  v &amp;lt;- rowMeans(v, na.rm = TRUE)
  
  # write to output file
  r_out &amp;lt;- writeValues(r_out, v, b$row[i])
}

# close files
r_out &amp;lt;- writeStop(r_out)
r_in &amp;lt;- readStop(r_in)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it, not particularly complicated! Let’s make sure it worked by comparing to the results from &lt;code&gt;calc()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cellStats(abs(r_mean- r_out), max, na.rm = TRUE)
#&amp;gt; [1] 2.98e-08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything looks good, the results are identical! Hopefully, this minimal example is a good template if you want to build your own raster processing functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parallel-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parallel processing&lt;/h2&gt;
&lt;p&gt;Breaking a raster up into blocks and processing each independently suggests another approach to more efficient raster processing: parallelization. Each block could be processed by a different core or node and the results will be collected in the output file. Fortunately, &lt;code&gt;raster&lt;/code&gt; has some nice tools for parallel raster processing. The function &lt;code&gt;clusterR()&lt;/code&gt; essentially takes an existing &lt;code&gt;raster&lt;/code&gt; function such as &lt;code&gt;calc()&lt;/code&gt; and runs it in parallel. Prior to using it, we need to Here’s how it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# start a cluster with four cores
beginCluster(n = 4)
t_parallel &amp;lt;- system.time({
  parallel_mean &amp;lt;- clusterR(r, fun = calc,
                            args = list(fun = mean, na.rm = TRUE))
})
endCluster()

# time for parallel calc
t_parallel[3]
#&amp;gt; elapsed 
#&amp;gt;    10.7
# time for sequential calc
t_ondisk[3]
#&amp;gt; elapsed 
#&amp;gt;    6.09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hmmm, there’s something odd going on here, it’s taking longer for the parallel version than the sequential version. I suspect I’ve messed something up in the parallel implementation. Let me know if you know what I’ve done wrong, perhaps it’s a topic for another post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fishnets and Honeycomb: Square vs. Hexagonal Spatial Grids</title>
      <link>/post/hexagonal-grids/</link>
      <pubDate>Thu, 14 Jan 2016 00:00:00 +0000</pubDate>
      <guid>/post/hexagonal-grids/</guid>
      <description>


&lt;p&gt;In spatial analysis, we often define &lt;a href=&#34;https://en.wikipedia.org/wiki/Grid_(spatial_index)&#34;&gt;grids&lt;/a&gt; of points or polygons to sample, index, or partition a study area. For example, we may want to overlay a study area with a grid of points as part of some regular spatial sampling scheme, divide a large region into smaller units for indexing purposes as with UTM grid zones, or slice the study area into subunits over which we summarize a spatial variable. In the latter scenario, the most common approach is to use a raster format, in which a grid of uniform square cells is overlayed on a study area and each cell is assigned a value for the spatial variables of interest. In ecology and conservation applications, variables may include number of individuals of a threatened species per grid cell, elevation, mean annual rainfall, or land use.&lt;/p&gt;
&lt;p&gt;From my experience, using square cells is by far the most common method for defining grids; however, other options are possible. In fact, any &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_tilings_by_convex_regular_polygons&#34;&gt;regular tesselation of the plane&lt;/a&gt; (i.e. the tiling of a plane with contiguous regular polygons of the same type), can act as a spatial grid. Tessellation is well studied mathematically and there are just &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_tilings_by_convex_regular_polygons#Regular_tilings&#34;&gt;three possible regular tesselations&lt;/a&gt;: equilateral triangles, squares, and regular hexagons. A forth option is a diamond pattern arising from merging pairs of equilateral triangles; however diamonds are not regular polygons. The following images from Wikipedia&lt;sup id=&#34;a1&#34;&gt;&lt;a href=&#34;#f1&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;#f2&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;#f3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; demonstrate these tessellations:&lt;/p&gt;
&lt;div style=&#34;text-align: center; display: inline;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;1-uniform_n11.svg&#34; style=&#34;width: 30%;&#34; /&gt;
&lt;img src=&#34;1-uniform_n5.svg&#34; style=&#34;width: 30%;&#34; /&gt;
&lt;img src=&#34;1-uniform_n1.svg&#34; style=&#34;width: 30%;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Recently I’ve seen a few instances of the use of &lt;strong&gt;hexagonal grids&lt;/strong&gt;, especially in systematic reserve design, and I’ve become curious about the benefits (and drawbacks) of using them compared to traditional square grids. In this post I’ll discuss the relative benefits and show how to generate different types of grids in R.&lt;/p&gt;
&lt;div id=&#34;comparing-benefits&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing Benefits&lt;/h1&gt;
&lt;p&gt;I begin by comparing the benefits of square and hexagonal grids. Most of these points come directly from &lt;a href=&#34;http://gis.stackexchange.com/questions/82362/what-are-the-benefits-of-hexagonal-sampling-polygons&#34;&gt;this excellent GIS StackExchange question&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;square-grids&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Square grids&lt;/h2&gt;
&lt;p&gt;Raster datasets are the most ubiquitous type of square grid used in GIS. The most notable benefits of this format compared to hexagonal grids are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simplicity of definition and data storage&lt;/strong&gt;: the only explicitly geographical information required to define a raster grid are the coordinates of the origin (e.g. bottom left corner), the cell size, and grid dimensions (i.e. number of cells in each direction). The attribute data can be stored as an aspatial matrix, and the geographical location of any cell can be derived given that cell’s position relative to the origin. This makes data storage and retrieval easier since the coordinates of the vertices of each grid cell are not explicitly stored.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease of resampling to different spatial scales&lt;/strong&gt;: increasing the spatial resolution of a square grid is just a matter of dividing each grid cell into four. Similarly, decreasing the spatial resolution only requires combining groups of four cells into one, typically with some algebraic operation to aggregate the attribute data to the coarser resolution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relationship between cells is given&lt;/strong&gt;: there is no need for computationally expensive spatial operations to determine distances or the adjacency relationship between cells.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combining raster layers is simple&lt;/strong&gt;: algebraic operations combining multiple raster layers built on the same template simplifies to matrix algebra; no spatial operations are required.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;hexagonal-grids&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hexagonal grids&lt;/h2&gt;
&lt;p&gt;Regular hexagons are the closest shape to a circle that can be used for the regular tessellation of a plane and they have additional symmetries compared to squares. These properties give rise to the following benefits.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reduced edge effects&lt;/strong&gt;: a hexagonal grid gives the lowest perimeter to area ratio of any regular tessellation of the plane. In practice, this means that edge effects are minimized when working with hexagonal grids. This is essentially the same reason &lt;a href=&#34;https://en.wikipedia.org/wiki/Honeycomb_conjecture&#34;&gt;beehives are built from hexagonal honeycomb&lt;/a&gt;: it is the arrangement that minimizes the amount of material used to create a lattice of cells with a given volume.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;All neighbours are identical&lt;/strong&gt;: square grids have two classes of neighbours, those in the cardinal directions that share an edge and those in diagonal directions that share a vertex. In contrast, a hexagonal grid cell has six identical neighbouring cells, each sharing one of the six equal length sides. Furthermore, the distance between centroids is the same for all neighbours.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better fit to curved surfaces&lt;/strong&gt;: when dealing with large areas, where the curvature of the earth becomes important, hexagons are better able to fit this curvature than squares. This is why soccer balls are constructed of hexagonal panels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;They look badass&lt;/strong&gt;: it can’t be denied that hexagonal grids look way more impressive than square grids!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;grids-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Grids in R&lt;/h1&gt;
&lt;div id=&#34;required-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(sp)
library(raster)
library(rgeos)
library(rgbif)
library(viridis)
library(gridExtra)
library(rasterVis)
set.seed(1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;study-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Study region&lt;/h2&gt;
&lt;p&gt;In the following demonstrations, I’ll use Sri Lanka as an example study area. The &lt;code&gt;getData()&lt;/code&gt; function from the &lt;code&gt;raster&lt;/code&gt; package downloads country boundaries from the &lt;a href=&#34;http://www.gadm.org/&#34;&gt;Global Administrative Areas (GADM) database&lt;/a&gt;. I clean this up a little by removing the attribute information and any polygons other than the main island of Sri Lanka.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;study_area &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;LKA&amp;quot;, level = 0, path = tempdir()) %&amp;gt;% 
  disaggregate %&amp;gt;% 
  geometry
study_area &amp;lt;- sapply(study_area@polygons, slot, &amp;quot;area&amp;quot;) %&amp;gt;% 
  {which(. == max(.))} %&amp;gt;% 
  study_area[.]
plot(study_area, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = TRUE, cex = 20)
text(81.5, 9.5, &amp;quot;Study Area:\nSri Lanka&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/study-region-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-grids&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating grids&lt;/h2&gt;
&lt;div id=&#34;hexagonal-grids-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hexagonal grids&lt;/h3&gt;
&lt;p&gt;There is no function in R that will directly generate a hexagonal grid of polygons covering a given region; however, it can be accomplished by first generating a hexagonal grid of points with &lt;code&gt;spsample&lt;/code&gt;, then converting this point grid to a grid of polygons with &lt;code&gt;HexPoints2SpatialPolygons&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;size &amp;lt;- 0.5
hex_points &amp;lt;- spsample(study_area, type = &amp;quot;hexagonal&amp;quot;, cellsize = size)
hex_grid &amp;lt;- HexPoints2SpatialPolygons(hex_points, dx = size)
plot(study_area, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = TRUE)
plot(hex_points, col = &amp;quot;black&amp;quot;, pch = 20, cex = 0.5, add = T)
plot(hex_grid, border = &amp;quot;orange&amp;quot;, add = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/hex-grid-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A few issues arise with this simple method:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;spsample&lt;/code&gt; generates a different grid of points each time it’s called because the grid offset is chosen randomly by default. This can be fixed by setting the offset parameter explicitly with &lt;code&gt;offset = c(0, 0)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Only cells whose centroid is fully within the study area polygon are created. By buffering the study area it’s possible to get full coverage by the grid, which is usually what is desired.&lt;/li&gt;
&lt;li&gt;In some cases it may be desirable to clip the grid to the study area polygon so that cells on the edge match the shape of the study area. This seems to often be the case when setting up a grid of planning units for systematic reserve design. For example, the official &lt;a href=&#34;http://www.uq.edu.au/marxan/intro-info&#34;&gt;Marxan tutorial&lt;/a&gt; takes this approach. Clipping can be performed using &lt;code&gt;rgeos::gIntersection()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The resolution of the grid is determined by the &lt;code&gt;cellsize&lt;/code&gt; parameter, which is the distance (&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;) between centroids of neighbouring cells. Other ways of defining cell size are the area (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;), side length (&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;), or radius (&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;), and these are all related by:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A = \frac{3\sqrt{3}}{2}s^2=2\sqrt{3}r^2=\frac{\sqrt{3}}{2}d^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I incorporate all these refinements into a function that generates hexagonal grids.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_grid &amp;lt;- function(x, cell_diameter, cell_area, clip = FALSE) {
  if (missing(cell_diameter)) {
    if (missing(cell_area)) {
      stop(&amp;quot;Must provide cell_diameter or cell_area&amp;quot;)
    } else {
      cell_diameter &amp;lt;- sqrt(2 * cell_area / sqrt(3))
    }
  }
  ext &amp;lt;- as(extent(x) + cell_diameter, &amp;quot;SpatialPolygons&amp;quot;)
  projection(ext) &amp;lt;- projection(x)
  # generate array of hexagon centers
  g &amp;lt;- spsample(ext, type = &amp;quot;hexagonal&amp;quot;, cellsize = cell_diameter, 
                offset = c(0.5, 0.5))
  # convert center points to hexagons
  g &amp;lt;- HexPoints2SpatialPolygons(g, dx = cell_diameter)
  # clip to boundary of study area
  if (clip) {
    g &amp;lt;- gIntersection(g, x, byid = TRUE)
  } else {
    g &amp;lt;- g[x, ]
  }
  # clean up feature IDs
  row.names(g) &amp;lt;- as.character(1:length(g))
  return(g)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using this function I generate a grid of &lt;span class=&#34;math inline&#34;&gt;\(625km^2\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(25km\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(25km\)&lt;/span&gt;) cells with and without clipping. This requires projecting the study area polygon to measure distance in kilometers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;study_area_utm &amp;lt;- CRS(&amp;quot;+proj=utm +zone=44 +datum=WGS84 +units=km +no_defs&amp;quot;) %&amp;gt;% 
  spTransform(study_area, .)
# without clipping
hex_grid &amp;lt;- make_grid(study_area_utm, cell_area = 625, clip = FALSE)
plot(study_area_utm, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = FALSE)
plot(hex_grid, border = &amp;quot;orange&amp;quot;, add = TRUE)
box()
# with clipping
hex_grid &amp;lt;- make_grid(study_area_utm, cell_area = 625, clip = TRUE)
plot(study_area_utm, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = FALSE)
plot(hex_grid, border = &amp;quot;orange&amp;quot;, add = TRUE)
box()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/nice-grid-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;square-grid&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Square grid&lt;/h3&gt;
&lt;p&gt;Creating and working with raster datasets in R is well covered elsewhere, for example in the vignettes for the &lt;code&gt;raster&lt;/code&gt; package, so I won’t delve too deeply into it. Briefly, &lt;code&gt;RasterLayer&lt;/code&gt; objects can easily be created that cover the extent of a &lt;code&gt;Spatial*&lt;/code&gt; object. I use a cell size of &lt;span class=&#34;math inline&#34;&gt;\(625km^2\)&lt;/span&gt; to match the above hexagonal grid, and fill the raster with binary data indicating whether cells are inside or outside the study area.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- raster(study_area_utm, resolution = 25)
r &amp;lt;- rasterize(study_area_utm, r, field = 1)
plot(r, col = &amp;quot;grey50&amp;quot;, axes = FALSE, legend = FALSE, bty=&amp;quot;n&amp;quot;, box=FALSE)
plot(study_area_utm, add = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/raster-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In addition to the raster formats defined in the &lt;code&gt;raster&lt;/code&gt; package, the &lt;code&gt;sp&lt;/code&gt; package offers several options for square grids. The class &lt;code&gt;SpatialPixels&lt;/code&gt; is used for partial grids (i.e. not every cell included) and stores the coordinates of all included cell centers. &lt;code&gt;SpatialGrid&lt;/code&gt; objects store full grids and do not store coordinates explicitly. Underlying both classes is the &lt;code&gt;GridTopology&lt;/code&gt; class, which stores the grid template (origin, cell size, and dimensions). I never use these classes since the &lt;code&gt;raster&lt;/code&gt; classes and methods are more intuitive and efficient.&lt;/p&gt;
&lt;p&gt;The final option from the &lt;code&gt;sp&lt;/code&gt; package is simply to store a square grid as polygons (&lt;code&gt;SpatialPolygons&lt;/code&gt; object), just as I did with the hexagonal grids above. In this case, I find the easiest way to define a grid of square polygons is to start with an empty &lt;code&gt;RasterLayer&lt;/code&gt; object, coerce it to &lt;code&gt;SpatialPolygons&lt;/code&gt;, and clip it as necessary. I incorporate this into the above grid generation function, which now creates hexagonal &lt;em&gt;or&lt;/em&gt; square grids as desired.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_grid &amp;lt;- function(x, type, cell_width, cell_area, clip = FALSE) {
  if (!type %in% c(&amp;quot;square&amp;quot;, &amp;quot;hexagonal&amp;quot;)) {
    stop(&amp;quot;Type must be either &amp;#39;square&amp;#39; or &amp;#39;hexagonal&amp;#39;&amp;quot;)
  }
  
  if (missing(cell_width)) {
    if (missing(cell_area)) {
      stop(&amp;quot;Must provide cell_width or cell_area&amp;quot;)
    } else {
      if (type == &amp;quot;square&amp;quot;) {
        cell_width &amp;lt;- sqrt(cell_area)
      } else if (type == &amp;quot;hexagonal&amp;quot;) {
        cell_width &amp;lt;- sqrt(2 * cell_area / sqrt(3))
      }
    }
  }
  # buffered extent of study area to define cells over
  ext &amp;lt;- as(extent(x) + cell_width, &amp;quot;SpatialPolygons&amp;quot;)
  projection(ext) &amp;lt;- projection(x)
  # generate grid
  if (type == &amp;quot;square&amp;quot;) {
    g &amp;lt;- raster(ext, resolution = cell_width)
    g &amp;lt;- as(g, &amp;quot;SpatialPolygons&amp;quot;)
  } else if (type == &amp;quot;hexagonal&amp;quot;) {
    # generate array of hexagon centers
    g &amp;lt;- spsample(ext, type = &amp;quot;hexagonal&amp;quot;, cellsize = cell_width, offset = c(0, 0))
    # convert center points to hexagons
    g &amp;lt;- HexPoints2SpatialPolygons(g, dx = cell_width)
  }
  
  # clip to boundary of study area
  if (clip) {
    g &amp;lt;- gIntersection(g, x, byid = TRUE)
  } else {
    g &amp;lt;- g[x, ]
  }
  # clean up feature IDs
  row.names(g) &amp;lt;- as.character(1:length(g))
  return(g)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting all four types of grids that &lt;code&gt;make_grid()&lt;/code&gt; can generate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hex - without clipping
hex_grid &amp;lt;- make_grid(study_area_utm, type = &amp;quot;hexagonal&amp;quot;, cell_area = 625, clip = FALSE)
plot(study_area_utm, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = FALSE)
plot(hex_grid, border = &amp;quot;orange&amp;quot;, add = TRUE)
box()
# hex - with clipping
hex_grid_c &amp;lt;- make_grid(study_area_utm, type = &amp;quot;hexagonal&amp;quot;, cell_area = 625, clip = TRUE)
plot(study_area_utm, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = FALSE)
plot(hex_grid_c, border = &amp;quot;orange&amp;quot;, add = TRUE)
box()
# square - without clipping
sq_grid &amp;lt;- make_grid(study_area_utm, type = &amp;quot;square&amp;quot;, cell_area = 625, clip = FALSE)
plot(study_area_utm, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = FALSE)
plot(sq_grid, border = &amp;quot;orange&amp;quot;, add = TRUE)
box()
# square - with clipping
sq_grid_c &amp;lt;- make_grid(study_area_utm, type = &amp;quot;square&amp;quot;, cell_area = 625, clip = TRUE)
plot(study_area_utm, col = &amp;quot;grey50&amp;quot;, bg = &amp;quot;light blue&amp;quot;, axes = FALSE)
plot(sq_grid_c, border = &amp;quot;orange&amp;quot;, add = TRUE)
box()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/square-polys-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-grids&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with grids&lt;/h2&gt;
&lt;p&gt;Once you’ve created a hexagonal grid, you’ll likely want to aggregate some data over the grid cells. Here I demonstrate three common aggregation tasks I often run into: aggregating points, polygons, or rasters. In these examples, I’ll use spatial data for Ecuador.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ecuador &amp;lt;- getData(name = &amp;quot;GADM&amp;quot;, country = &amp;quot;ECU&amp;quot;, level = 0, 
                   path = tempdir()) %&amp;gt;% 
  disaggregate %&amp;gt;% 
  geometry
# exclude gapalapos
ecuador &amp;lt;- sapply(ecuador@polygons, slot, &amp;quot;area&amp;quot;) %&amp;gt;% 
  {which(. == max(.))} %&amp;gt;% 
  ecuador[.]
# albers equal area for south america
ecuador &amp;lt;- spTransform(ecuador, CRS(
  paste(&amp;quot;+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60&amp;quot;,
        &amp;quot;+x_0=0 +y_0=0 +ellps=aust_SA +units=km +no_defs&amp;quot;)))
hex_ecu &amp;lt;- make_grid(ecuador, type = &amp;quot;hexagonal&amp;quot;, cell_area = 2500, clip = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;point-density&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Point density&lt;/h3&gt;
&lt;p&gt;It’s often necessary to summarize a set of point features (e.g. species occurrence points), over a grid by calculating the point density, i.e. the number of points within each grid cell. As an example, I’ll look at bird observations in Ecuador from &lt;a href=&#34;http://ebird.org/content/ebird/&#34;&gt;eBird&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ebird.org/&#34;&gt;eBird&lt;/a&gt; is online tool for birders to record their sightings. Each month millions of observations are entered into eBird globally, making it among the largest citizen science projects in history. The &lt;a href=&#34;http://www.gbif.org/&#34;&gt;Global Biodiversity Information Facility&lt;/a&gt; (GBIF) is a repository for biodiversity occurrence records. They store and provide access to hundreds of millions of records from thousands of sources, including eBird. The &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; package &lt;a href=&#34;https://ropensci.org/tutorials/rgbif_tutorial.html&#34;&gt;&lt;code&gt;rgbif&lt;/code&gt;&lt;/a&gt; provides a nice interface for importing GBIF records into R.&lt;/p&gt;
&lt;p&gt;I grab a subset of eBird sightings for 4 arbitrarily chosen bird families: tanagers (&lt;a href=&#34;https://en.wikipedia.org/wiki/Hummingbird&#34;&gt;Trochilidae&lt;/a&gt;), hummingbirds (&lt;a href=&#34;https://en.wikipedia.org/wiki/Tanager&#34;&gt;Thraupidae&lt;/a&gt;), herons (&lt;a href=&#34;https://en.wikipedia.org/wiki/Heron&#34;&gt;Ardeidae&lt;/a&gt;), and hawks (&lt;a href=&#34;https://en.wikipedia.org/wiki/Accipitridae&#34;&gt;Accipitridae&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bird_families &amp;lt;- c(&amp;quot;Trochilidae&amp;quot;, &amp;quot;Thraupidae&amp;quot;, &amp;quot;Ardeidae&amp;quot;, &amp;quot;Accipitridae&amp;quot;)
families &amp;lt;- data_frame(family = bird_families) %&amp;gt;% 
  group_by(family) %&amp;gt;% 
  do(name_suggest(q = .$family, rank = &amp;quot;family&amp;quot;)) %&amp;gt;% 
  filter(family == canonicalName) %&amp;gt;% 
  dplyr::select(family, key)
gb &amp;lt;- occ_search(taxonKey = families$key, country = &amp;quot;EC&amp;quot;, datasetKey = ebird_key, 
                 limit = 3000, return = &amp;quot;data&amp;quot;,
                 fields = c(&amp;quot;family&amp;quot;, &amp;quot;species&amp;quot;, &amp;quot;decimalLatitude&amp;quot;, &amp;quot;decimalLongitude&amp;quot;),
                 hasCoordinate = TRUE, hasGeospatialIssue = FALSE) %&amp;gt;% 
  rbind_all %&amp;gt;% 
  rename(lng = decimalLongitude, lat = decimalLatitude) %&amp;gt;% 
  as.data.frame
coordinates(gb) &amp;lt;- ~ lng + lat
projection(gb) &amp;lt;- projection(study_area)
gb &amp;lt;- spTransform(gb, projection(ecuador))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I summarize these sightings over the hexagonal grid to get point density, and plot the data in the form of a heat map.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fill_missing &amp;lt;- expand.grid(id = row.names(hex_ecu), 
                            family = bird_families, stringsAsFactors = FALSE)
point_density &amp;lt;- over(hex_ecu, gb, returnList = TRUE) %&amp;gt;% 
  plyr::ldply(.fun = function(x) x, .id = &amp;quot;id&amp;quot;) %&amp;gt;%
  mutate(id = as.character(id)) %&amp;gt;% 
  count(id, family) %&amp;gt;% 
  left_join(fill_missing, ., by = c(&amp;quot;id&amp;quot;, &amp;quot;family&amp;quot;)) %&amp;gt;%
  # log transform
  mutate(n = ifelse(is.na(n), -1, log10(n))) %&amp;gt;% 
  spread(family, n, fill = -1) %&amp;gt;% 
  SpatialPolygonsDataFrame(hex_ecu, .)
spplot(point_density, bird_families,
       main = &amp;quot;Ecuador eBird Sightings by Family&amp;quot;,
       col.regions = c(&amp;quot;grey20&amp;quot;, viridis(255)),
       colorkey = list(
         space = &amp;quot;bottom&amp;quot;,
         at = c(-0.1, seq(0, log10(1200), length.out = 255)),
         labels = list(
           at = c(-0.1, log10(c(1, 5, 25, 75, 250, 1200))),
           labels = c(0, 1, 5, 25, 75, 250, 1200)
           )
         ),
       xlim = bbexpand(bbox(point_density)[1, ], 0.04), 
       ylim = bbexpand(bbox(point_density)[2, ], 0.04),
       par.strip.text = list(col = &amp;quot;white&amp;quot;),
       par.settings = list(
         strip.background = list(col = &amp;quot;grey40&amp;quot;))
       )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/point-density-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ve used &lt;code&gt;spplot()&lt;/code&gt; here, and chosen to use a logarithmic scale, which means a big mess of legend parameters. Unfortunately, the data aren’t all that interesting, though I think the maps are pretty!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polygon-coverage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polygon coverage&lt;/h3&gt;
&lt;p&gt;Another common task, is determining the extent to which grid cells are covered by a polygon geometry. This could be in terms of absolute area covered or percent coverage. In the context of systematic reserve design, we may have species ranges as polygons and want to know the amount of each grid cell that is suitable habitat for each species. This can help highlight which cells are of highest conservation value.&lt;/p&gt;
&lt;p&gt;As a simple toy example, I use the boundary of Pastaza State.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pastaza &amp;lt;- getData(name = &amp;quot;GADM&amp;quot;, country = &amp;quot;ECU&amp;quot;, level = 1, 
                   path = tempdir()) %&amp;gt;%
  subset(NAME_1 == &amp;quot;Pastaza&amp;quot;) %&amp;gt;% 
  spTransform(projection(hex_ecu))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And calculate the degree to which it covers the grid cells. To intersect the polygons I use &lt;code&gt;gIntersection(byid = TRUE)&lt;/code&gt; from the &lt;code&gt;rgoes&lt;/code&gt; package. Note that with &lt;code&gt;byid = TRUE&lt;/code&gt;, the intersection is performed at the level of individual features within the geometry. For each resulting intersection polygon the feature ID is composed of the two source polygon IDs separated by a space. &lt;code&gt;gArea(byid = TRUE)&lt;/code&gt;, also from &lt;code&gt;rgeos&lt;/code&gt;, returns the area for each polygon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cell areas
hex_area &amp;lt;- make_grid(ecuador, type = &amp;quot;hexagonal&amp;quot;, cell_area = 2500, clip = TRUE)
hex_area &amp;lt;- gArea(hex_area, byid = T) %&amp;gt;% 
  data.frame(id = names(.), area = ., stringsAsFactors = FALSE) %&amp;gt;% 
  SpatialPolygonsDataFrame(hex_area, .)
hex_cover &amp;lt;- gIntersection(hex_area, pastaza, byid = TRUE) %&amp;gt;% 
  gArea(byid = TRUE) %&amp;gt;% 
  data.frame(id_both = names(.), cover_area = ., stringsAsFactors = FALSE) %&amp;gt;% 
  separate(id_both, &amp;quot;id&amp;quot;, extra = &amp;quot;drop&amp;quot;) %&amp;gt;% 
  merge(hex_area, ., by = &amp;quot;id&amp;quot;)
hex_cover$cover_area[is.na(hex_cover$cover_area)] &amp;lt;- 0
hex_cover$pct_cover &amp;lt;- 100 * hex_cover$cover_area / hex_cover$area&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I plot these two cover variables, again using &lt;code&gt;spplot()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# area
p1 &amp;lt;- spplot(hex_cover, &amp;quot;cover_area&amp;quot;, col = &amp;quot;white&amp;quot;, lwd = 0.5,
       main = expression(km^2),
       col.regions = plasma(256),
       par.settings = list(axis.line = list(col =  &amp;#39;transparent&amp;#39;)),
       colorkey = list(
         space = &amp;quot;bottom&amp;quot;,
         at = seq(0, 2500, length.out = 256),
         axis.line = list(col =  &amp;#39;black&amp;#39;))
       )
# percent cover
p2 &amp;lt;- spplot(hex_cover, &amp;quot;pct_cover&amp;quot;, col = &amp;quot;white&amp;quot;, lwd = 0.5,
       main = expression(&amp;quot;%&amp;quot;),
       col.regions = plasma(256),
       par.settings = list(axis.line = list(col =  &amp;#39;transparent&amp;#39;)),
       colorkey = list(
         space = &amp;quot;bottom&amp;quot;,
         at = seq(0, 100, length.out = 256),
         axis.line = list(col =  &amp;#39;black&amp;#39;))
       )
grid.arrange(p1, p2, ncol = 2, top = &amp;quot;Ecuador: Coverage by Pastaza State&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/cover-plot-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, these data are not very interesting, but the example is illustrative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;raster-aggregation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Raster aggregation&lt;/h3&gt;
&lt;p&gt;One of the most common operation I find myself doing with hexagonal grids is aggregating raster layers over the grid cells. For example, elevation or climate variables in raster format might be averaged over hexagonal grid cells, then used to parameterize a &lt;a href=&#34;https://en.wikipedia.org/wiki/Environmental_niche_modelling&#34;&gt;species distribution model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;getData()&lt;/code&gt; function from the &lt;code&gt;raster&lt;/code&gt; package provides access to elevation and climate raster datasets. As an example, I’ll use the SRTM elevation dataset, which has been aggregated to 1km resolution. First, I download, crop, and reproject this dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;srtm &amp;lt;- getData(&amp;#39;alt&amp;#39;, country = &amp;#39;ECU&amp;#39;, path = tempdir()) %&amp;gt;% 
  projectRaster(t_crop, to = raster(hex_ecu, res=1)) %&amp;gt;% 
  setNames(&amp;#39;elevation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To average this raster dataset over the hexagonal cells, I use the &lt;code&gt;extract()&lt;/code&gt; function from the &lt;code&gt;raster&lt;/code&gt; package. By default this function returns values from all raster cells that intersect with a given input geometry; however, with parameters &lt;code&gt;fun = mean&lt;/code&gt; and &lt;code&gt;sp = TRUE&lt;/code&gt; it will average the raster over each polygon and return a &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt; object with this information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hex_srtm &amp;lt;- extract(srtm, hex_ecu, fun = mean, na.rm = TRUE, sp = TRUE)
p1 &amp;lt;- levelplot(srtm, 
                col.regions = terrain.colors,
                margin = FALSE, scales = list(draw = FALSE),
                colorkey = list(
                  #space = &amp;quot;bottom&amp;quot;,
                  at = seq(0, 6000, length.out = 256),
                  labels = list(at = 1000 * 0:6, 
                                labels = format(1000 * 0:6, big.mark = &amp;quot;,&amp;quot;))
                )
      )
p2 &amp;lt;- spplot(hex_srtm,
             col.regions = terrain.colors(256),
             at = seq(0, 4000, length.out = 256),
             colorkey = list(
               labels = list(at = seq(0, 4000, 500), 
                             labels = format(seq(0, 4000, 500), big.mark = &amp;quot;,&amp;quot;))
             )
      )
grid.arrange(p1, p2, ncol = 2, top = &amp;quot;Ecuador SRTM Elevation (m)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-01-14-hexagonal-grids/index_files/figure-html/extract-1.png&#34; width=&#34;\textwidth&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The raster package makes this aggregation task extremely easy, just a single line of code! I’ve also used &lt;code&gt;levelplot()&lt;/code&gt; from &lt;code&gt;rasterVis&lt;/code&gt;, which provides a nice system for mapping raster data.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;p&gt;
&lt;strong&gt;Footnotes&lt;/strong&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;strong id=&#34;f1&#34;&gt;1&lt;/strong&gt;
“1-uniform n11” by Tomruen - Own work. Licensed under CC BY-SA 4.0 via Commons - &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:1-uniform_n11.svg&#34; class=&#34;uri&#34;&gt;https://commons.wikimedia.org/wiki/File:1-uniform_n11.svg&lt;/a&gt; &lt;a href=&#34;#a1&#34;&gt;↩︎&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;strong id=&#34;f2&#34;&gt;2&lt;/strong&gt;
“1-uniform n5” by Tomruen - Own work. Licensed under CC BY-SA 4.0 via Commons - &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:1-uniform_n5.svg&#34; class=&#34;uri&#34;&gt;https://commons.wikimedia.org/wiki/File:1-uniform_n5.svg&lt;/a&gt; &lt;a href=&#34;#a1&#34;&gt;↩︎&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;strong id=&#34;f3&#34;&gt;3&lt;/strong&gt;
“1-uniform n1” by Tomruen - Own work. Licensed under CC BY-SA 4.0 via Commons - &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:1-uniform_n1.svg&#34; class=&#34;uri&#34;&gt;https://commons.wikimedia.org/wiki/File:1-uniform_n1.svg&lt;/a&gt; &lt;a href=&#34;#a1&#34;&gt;↩︎&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
