[{"authors":["admin"],"categories":null,"content":"I write code and build tools to turn biodiversity data into conservation and science insights. As part of the eBird Status and Trends team at the Cornell Lab of Ornithology, I help develop species distribution models to estimate the occurrence, abundance, and population trends of bird species at a high spatiotemporal resolution using data generated by the citizen-science project eBird. I work on all facets of this project, from model testing and development to visualization of model results.\nTo ensure eBird data are widely and correctly used in science and conservation, I produce data products, build open source tools, develop educational resources, and teach workshops.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://strimas.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I write code and build tools to turn biodiversity data into conservation and science insights. As part of the eBird Status and Trends team at the Cornell Lab of Ornithology, I help develop species distribution models to estimate the occurrence, abundance, and population trends of bird species at a high spatiotemporal resolution using data generated by the citizen-science project eBird.","tags":null,"title":"Matt Strimas-Mackey","type":"authors"},{"authors":[],"categories":[],"content":" We work with a lot of huge raster datasets on the eBird Status \u0026amp; Trends project, and processing them is becoming a real bottleneck in our R workflow. For example, we make weekly estimates of bird abundance at 3 km resolution across the entire Western Hemisphere, which results in raster stacks with billions of cells! To produce seasonal abundance maps, we need to average the weekly layers across all weeks within each season using the raster function calc(), and it takes forever with these huge files! In this post, I’m going to try to understand how raster processes data and explore how this can be tweaked to improve computational efficiency.\nIn general, R holds objects in memory, which results in a limit to the size of objects that can be processed. This poses a problem for processing raster datasets, which can be much larger than the available system memory. The raster package addresses this by only storing references to raster files within its Raster* objects. Depending on the memory requirements for a given raster calculation, and the memory available, the package functions will either read the whole dataset into R for processing or process it in smaller chunks.\nLet’s start by importing an example dataset generated using the simulate_species() function from the prioritizr package. The raster has dimensions 1000x1000 and 9 layers.\nlibrary(raster) library(rasterVis) library(viridis) library(profmem) library(tidyverse) r \u0026lt;- stack(\u0026quot;data/large-raster.tif\u0026quot;) print(r) #\u0026gt; class : RasterStack #\u0026gt; dimensions : 1000, 1000, 1e+06, 9 (nrow, ncol, ncell, nlayers) #\u0026gt; resolution : 0.001, 0.001 (x, y) #\u0026gt; extent : 0, 1, 0, 1 (xmin, xmax, ymin, ymax) #\u0026gt; crs : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 #\u0026gt; names : large.raster.1, large.raster.2, large.raster.3, large.raster.4, large.raster.5, large.raster.6, large.raster.7, large.raster.8, large.raster.9 #\u0026gt; min values : 0, 0, 0, 0, 0, 0, 0, 0, 0 #\u0026gt; max values : 0.885, 0.860, 0.583, 0.744, 0.769, 0.428, 0.289, 0.579, 0.499 levelplot(r, col.regions = viridis, xlab = NULL, ylab = NULL, scales = list(draw = FALSE), names.attr = paste(\u0026quot;Band\u0026quot;, seq_len(nlayers(r))), maxpixels = 1e6) We can calculate the total number of values this raster can store and the associated memory requirements assuming 8 bytes per cell value. To calculate the actual memory used, we can override the default raster behavior and read the contents of the file into R using readAll().\nn_values \u0026lt;- ncell(r) * nlayers(r) # memory in mb mem_est \u0026lt;- 8 * n_values / 2^20 mem_act \u0026lt;- as.integer(object.size(readAll(r))) / 2^20 #\u0026gt; [1] \u0026quot;# values in raster: 9,000,000\u0026quot; #\u0026gt; [1] \u0026quot;Estimated size (MB): 68.7\u0026quot; #\u0026gt; [1] \u0026quot;Memory usage (MB): 68.8\u0026quot; #\u0026gt; used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) #\u0026gt; Ncells 2282143 122 4.15e+06 222 NA 3.83e+06 204 #\u0026gt; Vcells 45906401 350 1.47e+08 1118 102400 2.29e+08 1745 So we were fairly close in our estimates, looks like it takes a little under 70 Mb of memory to hold this object in the R session.\nProcessing rasters Let’s apply the calc() function to this dataset to calculate the cell-wise mean across layers.\nr_mean \u0026lt;- calc(r, mean, na.rm = TRUE) This is essentially to equivalent of apply() for a array with 3 dimensions, e.g.:\na \u0026lt;- array(runif(27), dim = c(3, 3, 3)) apply(a, 1:2, mean) #\u0026gt; [,1] [,2] [,3] #\u0026gt; [1,] 0.598 0.545 0.484 #\u0026gt; [2,] 0.570 0.275 0.460 #\u0026gt; [3,] 0.588 0.618 0.545 But what raster is doing in calc() is a little different and depends on the memory requirements of the calculation. We can use the function canProccessInMemory() to test whether a Raster* object can be loaded into memory for processing or not. We’ll use verbose = TRUE to get some additional information.\ncanProcessInMemory(r, verbose = TRUE) #\u0026gt; memory stats in GB #\u0026gt; mem available: 1.78 #\u0026gt; 60% : 1.07 #\u0026gt; mem needed : 0.27 #\u0026gt; max allowed : 4.66 (if available) #\u0026gt; [1] TRUE This tells me how much free memory I have available on my computer and how much memory is required for this Raster* object. We don’t want raster eating up all our memory, so raster has two user adjustable options to specify the maximum amount of memory it will use in bytes or relative to the available memory. These values default to 5 billion bytes (4.66 GB) and 60%, respectively, but can be adjusted with rasterOptions().\nraster functions call canProccessInMemory() when they’re invoked, then use a different approach for processing depending on the results:\n canProcessInMemory(r) == TRUE: read the entire object into the R session, then process all at once similar to apply(). canProcessInMemory(r) == FALSE: process the raster in blocks of rows, each of which is small enough to store in memory. This approach requires that the output raster object is saved in a file. Blocks of rows are read from the input files, processed in R, then written to the output file, and this is done iteratively for all the blocks until the whole raster is processed.  One wrinkle to this is that each raster function has different memory requirements. This is dealt with using the n argument to canProccessInMemory(), which specifies the number of copies of the Raster* object’s cell values that the function needs to have in memory. Specifically, the estimated memory requirement in bytes is 8 * n * ncell(r) * nlayers(r). Let’s see how different values of n affect whether a raster can be processed in memory:\ntibble(n = c(1, 10, 20, 30, 40, 50, 60)) %\u0026gt;% mutate(process_in_mem = map_lgl(n, canProcessInMemory, x = r)) #\u0026gt; # A tibble: 7 x 2 #\u0026gt; n process_in_mem #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; #\u0026gt; 1 1 TRUE #\u0026gt; 2 10 TRUE #\u0026gt; 3 20 FALSE #\u0026gt; 4 30 FALSE #\u0026gt; 5 40 FALSE #\u0026gt; 6 50 FALSE #\u0026gt; 7 60 FALSE So, even though I called this a “large” raster, R can still handle processing it in memory until we get to requiring a fairly large number of copies, at which time, the raster will switch to being processed in blocks. For reason I don’t fully understand, the source code of the calc() function suggests that it’s using n = 2 * (nlayers(r) + 1), which is 20, so calc() is processing this raster in memory on my system. Indeed, we can confirm that the result of this calculation are stored in a memory with inMemory().\ninMemory(r_mean) #\u0026gt; [1] FALSE What’s the point of going to all this trouble? If a raster can be processed in blocks to reduce memory usage, why not do it all the time? The issue is that processing in memory is much faster than processing in blocks and having to write to a file. We can see this by forcing calc() to process on disk in blocks by setting rasterOptions(todisk = TRUE).\n# in memory rasterOptions(todisk = FALSE) t_inmem \u0026lt;- system.time(calc(r, mean, na.rm = TRUE)) print(t_inmem) #\u0026gt; user system elapsed #\u0026gt; 5.842 0.405 6.348 # on disk rasterOptions(todisk = TRUE) t_ondisk \u0026lt;- system.time(calc(r, mean, na.rm = TRUE)) print(t_ondisk) #\u0026gt; user system elapsed #\u0026gt; 5.566 0.391 5.998 rasterOptions(todisk = FALSE) So, we see a -5.5% increase in efficiency by processing in memory. The profmem package can gives us some information on the different amounts of memory used for the two approaches. Specifically, we can estimate the maximum amount of memory used at any one time by calc().\n# in memory rasterOptions(todisk = FALSE) m_inmem \u0026lt;- max(profmem(calc(r, mean, na.rm = TRUE))$bytes, na.rm = TRUE) # on disk rasterOptions(todisk = TRUE) m_ondisk \u0026lt;- max(profmem(calc(r, mean, na.rm = TRUE))$bytes, na.rm = TRUE) rasterOptions(todisk = FALSE) #\u0026gt; [1] \u0026quot;In memory (MB): 17\u0026quot; #\u0026gt; [1] \u0026quot;On disk (MB): 17\u0026quot; So, it’s clear that different ways of processing Raster* objects affects both the processing time and resource use.\n raster options The raster package has a few options that can adjusted to tweak how functions process data. Let’s take a look at the default values:\nrasterOptions() #\u0026gt; format : raster #\u0026gt; datatype : FLT4S #\u0026gt; overwrite : FALSE #\u0026gt; progress : none #\u0026gt; timer : FALSE #\u0026gt; chunksize : 1e+08 #\u0026gt; maxmemory : 5e+09 #\u0026gt; memfrac : 0.6 #\u0026gt; tmpdir : /var/folders/mg/qh40qmqd7376xn8qxd6hm5lwjyy0h2/T//RtmpsJgyy5/raster// #\u0026gt; tmptime : 168 #\u0026gt; setfileext : TRUE #\u0026gt; tolerance : 0.1 #\u0026gt; standardnames : TRUE #\u0026gt; warn depracat.: TRUE #\u0026gt; header : none The options relevant to memory and processing are:\n maxmemory: the maximum amount of memory (in bytes) to use for a given operation, defaults to 5 billion bytes (4.66 GB). memfrac: the maximum proportion of the available memory to use for a given operation, defaults to 60%. chunksize: the maximum size (in bytes) of individual chunks of data to read/write when a raster is being processed in blocks, defaults to 100 million bytes (0.1 GB). todisk: used to force processing on disk in blocks.  For example, we can adjust chunksize to force calc() to process our raster stack in smaller pieces. Note that raster actually ignores user specified values of chunksize if they’re below \\(10^5\\), so I’ll have to do something sketchy and overwrite an internal raster function to allow this.\n# hack raster internal function cs_orig \u0026lt;- raster:::.chunk cs_hack \u0026lt;- function(x) getOption(\u0026quot;rasterChunkSize\u0026quot;) assignInNamespace(\u0026quot;.chunk\u0026quot;, cs_hack, ns = \u0026quot;raster\u0026quot;) # use 1 kb chunks rasterOptions(chunksize = 1000, todisk = TRUE) t_smallchunks \u0026lt;- system.time(calc(r, mean, na.rm = TRUE)) # undo the hack assignInNamespace(\u0026quot;.chunk\u0026quot;, cs_orig, ns = \u0026quot;raster\u0026quot;) rasterOptions(default = TRUE) Processing in smaller chunks resulted in a 54.9% decrease in efficiency compared to the default chunk size. All this suggests to me that, when dealing with large rasters, it makes sense to increase maxmemory as much as feasible given the memory available on your system; the default value of ~ 1 GB is quite small for a modern computer. Then, once you get to a point where the raster has to be processed in blocks, increase chunksize to take advantage of as much memory as you have available.\n Processing in blocks I want to take a quick detour to understand exactly how raster processes data in blocks. Looking at the source code of the calc() function gives a template for how this is done. A few raster functions help with this:\n blockSize() suggests a sensible way to break up a Raster* object for processing in blocks. The raster objects always uses a set of entire rows as blocks, so this function gives the starting row numbers of each block. readStart() opens a file on disk for reading. getValues() reads a block of data, defined by the starting row and number of rows. readStop() closes the input file. writeStart() opens a file on disk for writing the results of our calculations to. writeValues() writes a block of data to a file, starting at a given row. writeStop() closes the output file.  Let’s set things up to replicate what calc() does. First we need to determine how to dive the input raster up into blocks for processing:\n# file paths f_in \u0026lt;- \u0026quot;data/large-raster.tif\u0026quot; f_out \u0026lt;- tempfile(fileext = \u0026quot;.tif\u0026quot;) # input and output rasters r_in \u0026lt;- stack(f_in) r_out \u0026lt;- raster(r_in) # blocks b \u0026lt;- blockSize(r_in) print(b) #\u0026gt; $row #\u0026gt; [1] 1 251 501 751 #\u0026gt; #\u0026gt; $nrows #\u0026gt; [1] 250 250 250 250 #\u0026gt; #\u0026gt; $n #\u0026gt; [1] 4 So, blockSize() is suggesting we break the file up into 4 blocks (b$n) of 250 (b$nrows) each, and b$rows gives us the starting row value for each block. Now we open the input and output files, process the blocks iteratively, reading and writing as necessary, then close the files.\n# open files r_in \u0026lt;- readStart(r_in) r_out \u0026lt;- writeStart(r_out, filename = f_out) # loop over blocks for (i in seq_along(b$row)) { # read values for block # format is a matrix with rows the cells values and columns the layers v \u0026lt;- getValues(r_in, row = b$row[i], nrows = b$nrows[i]) # mean cell value across layers v \u0026lt;- rowMeans(v, na.rm = TRUE) # write to output file r_out \u0026lt;- writeValues(r_out, v, b$row[i]) } # close files r_out \u0026lt;- writeStop(r_out) r_in \u0026lt;- readStop(r_in) That’s it, not particularly complicated! Let’s make sure it worked by comparing to the results from calc().\ncellStats(abs(r_mean- r_out), max, na.rm = TRUE) #\u0026gt; [1] 0 Everything looks good, the results are identical! Hopefully, this minimal example is a good template if you want to build your own raster processing functions.\n Parallel processing Breaking a raster up into blocks and processing each independently suggests another approach to more efficient raster processing: parallelization. Each block could be processed by a different core and node and the results will be collected in the output file. Fortunately, raster has some nice tools for parallel raster processing. The function clusterR() essentially takes an existing raster function such as calc() and runs it in parallel. Prior to using it, we need to Here’s how it works:\n# start a cluster with four cores beginCluster(n = 4) t_parallel \u0026lt;- system.time({ parallel_mean \u0026lt;- clusterR(r, fun = calc, args = list(fun = mean, na.rm = TRUE)) }) endCluster() # time for parallel calc t_parallel[3] #\u0026gt; elapsed #\u0026gt; 11.5 # time for sequential calc t_ondisk[3] #\u0026gt; elapsed #\u0026gt; 6 Hmmm, there’s something odd going on here, it’s taking longer for the parallel version than the sequential version. I suspect I’ve messed something up in the parallel implementation. Let me know if you know what I’ve done wrong, perhaps it’s a topic for another post.\n ","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584734249,"objectID":"881ec18f566f49f9fc682c88b6349bd6","permalink":"https://strimas.com/post/processing-large-rasters-in-r/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/processing-large-rasters-in-r/","section":"post","summary":"Investigating how raster data are processed in R to improve the efficiency of processing large raster files.","tags":["R","Spatial"],"title":"Processing Large Rasters in R","type":"post"},{"authors":null,"categories":null,"content":"","date":1584576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584576000,"objectID":"f091f3cf08112c8397af723c8749259e","permalink":"https://strimas.com/project/ebirdst/","publishdate":"2020-03-19T00:00:00Z","relpermalink":"/project/ebirdst/","section":"project","summary":"Linking bird sightings from [eBird](https://ebird.org/) with habitat information from satellites to model when and where birds occur. This project estimates the distribution, abundance, and trends of bird populations at high spatial and temporal resolution across the entire Western Hemisphere.","tags":["eBird"],"title":"eBird Status \u0026 Trends","type":"project"},{"authors":null,"categories":null,"content":"","date":1584489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584489600,"objectID":"6135792b3057c9339296f1913bddd9cf","permalink":"https://strimas.com/project/ebp/","publishdate":"2020-03-18T00:00:00Z","relpermalink":"/project/ebp/","section":"project","summary":"This open source book outlines a set of best practices for using eBird data to estimate species distributions in R. It covers accessing and pre-processing eBird data, preparing satellite-derived habitat variables for use as model covariates, and best practices for model encounter rate, occupancy, and abundance using eBird data.","tags":["eBird","R"],"title":"eBird Best Practices","type":"project"},{"authors":null,"categories":null,"content":"","date":1584403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584403200,"objectID":"d58ab29a63121d596b5f2b95d5a0c814","permalink":"https://strimas.com/project/rebirdst/","publishdate":"2020-03-17T00:00:00Z","relpermalink":"/project/rebirdst/","section":"project","summary":"This R package provides access to [eBird Status and Trends](https://ebird.org/science/status-and-trends) data for over 600 North American bird species. These data include weekly estimates of relative abundance and occurrence at high spatial resolution across the entire Western Hemisphere.","tags":["eBird","R"],"title":"ebirdst","type":"project"},{"authors":null,"categories":null,"content":"","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"cae32be0b844730580e5f03039aa117b","permalink":"https://strimas.com/project/auk/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/project/auk/","section":"project","summary":"An R package to access and analyze bird observation data from [eBird](https://ebird.org/), the largest biodiversity-related citizen science project in the world. Observations can be extracted based on a variety of taxonomic, spatial, and temporal filters. `auk` also includes tools for importing and processing eBird data in R.","tags":["eBird","R"],"title":"auk","type":"project"},{"authors":null,"categories":null,"content":"","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584230400,"objectID":"5c0dccafa0dd760c9662a812ff07e576","permalink":"https://strimas.com/project/prioritizr/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/project/prioritizr/","section":"project","summary":"The `prioritizr` R package provide a flexible interface for building and solving systematic conservation prioritization problems using integer linear programming (ILP). It supports a broad range of objectives, constraints, and penalties that can be used to custom-tailor prioritization problems to the specific needs of a conservation planning exercise. For more details, watch [this talk](https://www.youtube.com/watch?v=da7r3Qyn6ag) by lead developer [Jeff Hanson](https://jeffrey-hanson.com/).","tags":["R"],"title":"prioritizr","type":"project"},{"authors":["Matt Strimas-Mackey"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://strimas.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" In this post, I’ll explore using R to analyze dynamical systems. Using the Lotka-Volterra predator prey model as a simple case-study, I use the R packages deSolve to solve a system of differential equations and FME to perform a sensitivity analysis.\nlibrary(tidyverse) library(deSolve) library(FME) Lotka-Volterra model The Lotka-Volterra model describes the dynamics of a two-species system in which one is a predator and the other is its prey. The equations governing the dynamics of the prey (with density \\(x\\) and predator (with density \\(y\\)) are:\n\\[ \\begin{aligned} \\frac{dx}{dt} \u0026amp; = \\alpha x - \\beta xy \\\\ \\frac{dy}{dt} \u0026amp; = \\delta \\beta xy - \\gamma y \\end{aligned} \\]\nwhere \\(\\alpha\\) is the (exponential) growth rate of the prey in the absence of predation, \\(\\beta\\) is the predation rate or predator search efficiency, \\(\\delta\\) describes the predator food conversion efficiency, and \\(\\gamma\\) is the predator mortality.\n Solving the ODE Given a set of initial conditions and parameter estimates, deSolve::ode() can be used to evolve a dynamical system described by a set of ODEs. I start by defining parameters, as a named list, and the initial state, as a vector. For the initial state, it is the order that matters not the names.\n# parameters pars \u0026lt;- c(alpha = 1, beta = 0.2, delta = 0.5, gamma = 0.2) # initial state init \u0026lt;- c(x = 1, y = 2) # times times \u0026lt;- seq(0, 100, by = 1) Next, I need to define a function that computes the derivatives in the ODE system at a given point in time.\nderiv \u0026lt;- function(t, state, pars) { with(as.list(c(state, pars)), { d_x \u0026lt;- alpha * x - beta * x * y d_y \u0026lt;- delta * beta * x * y - gamma * y return(list(c(x = d_x, y = d_y))) }) } lv_results \u0026lt;- ode(init, times, deriv, pars) The vignette for FME suggets rolling all this into a function as follows. This function will become the input for the FME sensitivity analysis.\nlv_model \u0026lt;- function(pars, times = seq(0, 50, by = 1)) { # initial state state \u0026lt;- c(x = 1, y = 2) # derivative deriv \u0026lt;- function(t, state, pars) { with(as.list(c(state, pars)), { d_x \u0026lt;- alpha * x - beta * x * y d_y \u0026lt;- delta * beta * x * y - gamma * y return(list(c(x = d_x, y = d_y))) }) } # solve ode(y = state, times = times, func = deriv, parms = pars) } lv_results \u0026lt;- lv_model(pars = pars, times = seq(0, 50, by = 0.25)) The ouput of ode() is a matrix with one column for each state variable. I convert this to a data frame and plot the evolution of the system over time.\nlv_results %\u0026gt;% data.frame() %\u0026gt;% gather(var, pop, -time) %\u0026gt;% mutate(var = if_else(var == \u0026quot;x\u0026quot;, \u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;)) %\u0026gt;% ggplot(aes(x = time, y = pop)) + geom_line(aes(color = var)) + scale_color_brewer(NULL, palette = \u0026quot;Set1\u0026quot;) + labs(title = \u0026quot;Lotka-Volterra predator prey model\u0026quot;, subtitle = paste(names(pars), pars, sep = \u0026quot; = \u0026quot;, collapse = \u0026quot;; \u0026quot;), x = \u0026quot;Time\u0026quot;, y = \u0026quot;Population density\u0026quot;) This choice of paramters leads to periodic dynamics in which the prey population initially increases, leading to an abundance of food for the predators. The predators increase in response (lagging the prey population), eventually overwhelming the prey population, which crashes. This in turn causes the predators to crash, and the cycle repeats. The period of these dynamics is about 15 seconds, with the predators lagging the prey by about a second.\n Sensitivity analysis A sensitivity analysis examines how changes in the parameters underlying a model affect the behaviour of the system. It can help us understand the impact of uncertainty in parameter estimates. The FME vignette covers two types of sensitivity analyses: global and local.\nGlobal sensitivity According to the FME vingette, in a global sensitivity analysis certain parameters (the sensitivity parameters) are varied over a large range, and the effect on model output variables (the sensitivity variables) is measured. To accomplish this, a distribution is defined for each sensitivity parameter and the model is run multiple times, each time drawing values for the sensistivity parameters from their distribution. The sensitivity variables are recorded for each iteration over a range of times. The function sensRange() carries out global sensitivity analyses.\nI’ll look at the sensitivity of the populations to the growth rate (\\(\\alpha\\)) and the predation rate (\\(\\beta\\)). Defining the sensitivity parameter distributions requires providing a data frame in which the first column is the minimum value, the second column the maximum, and the row names are the parameter names.\npar_ranges \u0026lt;- data.frame(min = c(0.75, 0.15), max = c(1.25, 0.25), row.names = c(\u0026quot;alpha\u0026quot;, \u0026quot;beta\u0026quot;)) par_ranges #\u0026gt; min max #\u0026gt; alpha 0.75 1.25 #\u0026gt; beta 0.15 0.25 Now I use sensRange() to solve the models over the range of parameters. The argument dist = \"grid\" sets the sensitivity parameter distribution to a regular grid of values, sensvar defines which variables are the sensitivity variables (i.e. the ones whose time series will be returned in the output), num is the number of iterations and therefore the number of different sensistivity parameter values (note that if there are \\(k\\) sensitivity parameters, num must have an integer \\(k\\)th root), and times is the time series over which to evaluate the model.\nThe output of this function is a data frame with rows corresponding to the different sensitivity parameter values and columns corresponding to the combination of time steps and sensitivity variables. So, for n time steps, there are n columns for each sensitivity variable. First I run a simple sensitivity analysis to aid examination of the output.\nlv_glob_sens \u0026lt;- sensRange(func = lv_model, parms = pars, dist = \u0026quot;grid\u0026quot;, sensvar = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), parRange = par_ranges, num = 4, times = seq(0, 1, by = 0.5)) lv_glob_sens #\u0026gt; alpha beta x0 x0.5 x1 y0 y0.5 y1 #\u0026gt; 1 0.75 0.15 1 1.26 1.59 2 1.89 1.80 #\u0026gt; 2 1.25 0.15 1 1.61 2.62 2 1.90 1.86 #\u0026gt; 3 0.75 0.25 1 1.14 1.30 2 1.93 1.89 #\u0026gt; 4 1.25 0.25 1 1.46 2.14 2 1.95 1.97 Here variables such as x0.5 refer to the values of \\(x\\) at \\(t=0.5\\). FME provides a plot() method for sensRange objects, which adds envelopes to the variables showing the range and mean ± standard deviation. Now I run a more realistic sensitivity analysis and produce the plots. Note that summary() must be called before plot() to get the desired plots.\nlv_glob_sens \u0026lt;- sensRange(func = lv_model, parms = pars, dist = \u0026quot;grid\u0026quot;, sensvar = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), parRange = par_ranges, num = 100, times = seq(0, 50, by = 0.25)) lv_glob_sens %\u0026gt;% summary() %\u0026gt;% plot(main = c(\u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;), xlab = \u0026quot;Time\u0026quot;, ylab = \u0026quot;Population density\u0026quot;, col = c(\u0026quot;lightblue\u0026quot;, \u0026quot;darkblue\u0026quot;)) mtext(\u0026quot;Sensitivity to alpha and beta\u0026quot;, outer = TRUE, line = -1.5, side = 3, cex = 1.25) To actually work with these data, I’ll transform the data frame from wide to long format using tidyr. gather() converts from wide to long format, then separate() splits column names x1.5 into two fields: one identifying the variable (x) and one specifying the time step (1.5).\nlv_sens_long \u0026lt;- lv_glob_sens %\u0026gt;% gather(key, abundance, -alpha, -beta) %\u0026gt;% separate(key, into = c(\u0026quot;species\u0026quot;, \u0026quot;t\u0026quot;), sep = 1) %\u0026gt;% mutate(t = parse_number(t)) %\u0026gt;% select(species, t, alpha, beta, abundance) head(lv_sens_long) #\u0026gt; species t alpha beta abundance #\u0026gt; 1 x 0 0.750 0.15 1 #\u0026gt; 2 x 0 0.806 0.15 1 #\u0026gt; 3 x 0 0.861 0.15 1 #\u0026gt; 4 x 0 0.917 0.15 1 #\u0026gt; 5 x 0 0.972 0.15 1 #\u0026gt; 6 x 0 1.028 0.15 1 glimpse(lv_sens_long) #\u0026gt; Observations: 40,200 #\u0026gt; Variables: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;,… #\u0026gt; $ t \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #\u0026gt; $ alpha \u0026lt;dbl\u0026gt; 0.750, 0.806, 0.861, 0.917, 0.972, 1.028, 1.083, 1.139, 1.1… #\u0026gt; $ beta \u0026lt;dbl\u0026gt; 0.150, 0.150, 0.150, 0.150, 0.150, 0.150, 0.150, 0.150, 0.1… #\u0026gt; $ abundance \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… Now, I can, for example, recreate the above plot with ggplot2. First, I summarize the data to calculate the envelopes, then I plot.\nlv_sens_summ \u0026lt;- lv_sens_long %\u0026gt;% group_by(species, t) %\u0026gt;% summarize(a_mean = mean(abundance), a_min = min(abundance), a_max = max(abundance), a_sd = sd(abundance)) %\u0026gt;% ungroup() %\u0026gt;% mutate(a_psd = a_mean + a_sd, a_msd = a_mean - a_sd, species = if_else(species == \u0026quot;x\u0026quot;, \u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;), species = factor(species, levels = c(\u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;))) ggplot(lv_sens_summ, aes(x = t, group = species)) + # mean+-sd geom_ribbon(aes(ymin = a_msd, ymax = a_psd, fill = species), alpha = 0.2) + # mean geom_line(aes(y = a_mean, color = species)) + labs(title = \u0026quot;Sensitivity to alpha and beta (mean ± sd)\u0026quot;, subtitle = \u0026quot;alpha = [0.75, 1.25]; beta = [0.15, 0.25]\u0026quot;, x = \u0026quot;Time\u0026quot;, y = \u0026quot;Population density\u0026quot;) + scale_color_brewer(NULL, palette = \u0026quot;Set1\u0026quot;) + scale_fill_brewer(NULL, palette = \u0026quot;Set1\u0026quot;) In this format, it’s also easy to fix one of the values for a sensitivity parameter and only vary the other one.\nlv_sens_summ \u0026lt;- lv_sens_long %\u0026gt;% # fix beta at 0.15 filter(beta == 0.15) %\u0026gt;% group_by(species, t) %\u0026gt;% summarize(a_mean = mean(abundance), a_min = min(abundance), a_max = max(abundance), a_sd = sd(abundance)) %\u0026gt;% ungroup() %\u0026gt;% mutate(a_psd = a_mean + a_sd, a_msd = a_mean - a_sd, species = if_else(species == \u0026quot;x\u0026quot;, \u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;), species = factor(species, levels = c(\u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;))) ggplot(lv_sens_summ, aes(x = t, group = species)) + # mean+-sd geom_ribbon(aes(ymin = a_msd, ymax = a_psd, fill = species), alpha = 0.2) + # mean geom_line(aes(y = a_mean, color = species)) + labs(title = \u0026quot;Sensitivity to alpha at fixed beta (mean ± sd)\u0026quot;, subtitle = \u0026quot;alpha = [0.75, 1.25]; beta = 0.15\u0026quot;, x = \u0026quot;Time\u0026quot;, y = \u0026quot;Population density\u0026quot;) + scale_color_brewer(NULL, palette = \u0026quot;Set1\u0026quot;) + scale_fill_brewer(NULL, palette = \u0026quot;Set1\u0026quot;)  Local sensitivity analysis According to the FME vingette, in a local sensitivity analysis, “the effect of a parameter value in a very small region near its nominal value is estimated”. The method used by FME is to calculate a matrix of sensitivity functions, \\(S_{i,j}\\), defined by:\n\\[ S_{i,j} = f(v_i, p_i) = \\frac{dv_i}{dp_j} \\frac{s_{p_j}}{s_{v_i}} \\]\nwhere \\(v_i\\) is a sensitivity variable \\(i\\) (which is dependent on time \\(t\\)), \\(p_j\\) is sensitivity parameter \\(j\\), and \\(s_{v_i}\\) and \\(s_{p_j}\\) are scaling factors for variables and parameters, respectively. By default, FME takes the scaling values to be equal to the underlying quantities, in which case the above equation simplifies to:\n\\[ S_{i,j} = f(v_i, p_i) = \\frac{dv_i}{dp_j} \\frac{p_j}{v_i} \\]\nThe function sensFun() is used to numerically estimate these sensitivity functions at a series of time steps. The arguments sensvar and senspar are used to define which variables and parameters, respectively, will be investigated in the sensitivity analysis. By default, all variables are parameters are included. The arguments varscale and parscale define the scaling factors; however, for now, I’ll leave them blank, which sets them to the underlying quantities as in the above equation.\nIn practice, sensFun() works by applying a small perturbation, \\(\\delta_j\\), to parameter \\(j\\), solving the model for a range of time steps to determine \\(v_i\\), then taking the ratio of the changes to the parameters and variables. The perturbation is defined by the argument tiny as \\(\\delta_j = \\text{max}(tiny, tiny * p_j)\\). tiny defaults to \\(10^{-8}\\).\nTo test that sensFun() is doing what I think it is, I’ll implement a version of it. For simplicity, I’ll only consider the variable \\(x\\) (prey density):\nsen_fun \u0026lt;- function(fun, pars, times, tiny = 1e-8) { # the unperturbed values, just x v_unpert \u0026lt;- fun(pars, times)[, \u0026quot;x\u0026quot;] # loop over parameters, pertuburbing each in turn s_ij \u0026lt;- matrix(NA, nrow = length(times), ncol = (1 + length(pars))) s_ij[, 1] \u0026lt;- times colnames(s_ij) \u0026lt;- c(\u0026quot;t\u0026quot;, names(pars)) for (j in seq_along(pars)) { # perturb the ith parameter delta \u0026lt;- max(tiny, abs(tiny * pars[j])) p_pert \u0026lt;- pars p_pert[j] \u0026lt;- p_pert[j] + delta # solve model v_pert \u0026lt;- fun(pars = p_pert, times = times)[, \u0026quot;x\u0026quot;] # calculate the resulting difference in variables at each timestep, just x delta_v \u0026lt;- (v_pert - v_unpert) # finally, calculate the sensitivity function at each time step s_ij[, j + 1] \u0026lt;- (delta_v / delta) * (pars[j] / v_unpert) } return(s_ij) } Now I compare this implementation to the actual results.\ntest_pars \u0026lt;- c(alpha = 1.5, beta = 0.2, delta = 0.5, gamma = 0.2) sen_fun(lv_model, pars = test_pars, times = 0:2) #\u0026gt; t alpha beta delta gamma #\u0026gt; [1,] 0 0.00 0.000 0.000 0.0000 #\u0026gt; [2,] 1 1.48 -0.415 -0.029 0.0386 #\u0026gt; [3,] 2 2.69 -0.966 -0.210 0.1655 sensFun(lv_model, parms = test_pars, sensvar = \u0026quot;x\u0026quot;, times = 0:2) #\u0026gt; x var alpha beta delta gamma #\u0026gt; 1 0 x 0.00 0.000 0.000 0.0000 #\u0026gt; 2 1 x 1.48 -0.415 -0.029 0.0386 #\u0026gt; 3 2 x 2.69 -0.966 -0.210 0.1655 A perfect match! Now that I know what sensFun() is actually doing, I’ll put it to use to solve the original LV model. One difference here is that I’ll consider both variables as sensitivity variables and the results for each variable will be stacked rowwise. In addition, in the FME documentation, \\(s_{v_i}\\) is set to 1, which is on the order of the actual variable values, but has the benefit of being constant over time. I’ll do the same here.\nlv_loc_sens \u0026lt;- sensFun(lv_model, parms = pars, varscale = 1, times = seq(0, 50, by = 0.25)) head(lv_loc_sens) #\u0026gt; x var alpha beta delta gamma #\u0026gt; 1 0.00 x 0.000 0.000 0.00000 0.00000 #\u0026gt; 2 0.25 x 0.291 -0.116 -0.00151 0.00286 #\u0026gt; 3 0.50 x 0.677 -0.272 -0.00729 0.01316 #\u0026gt; 4 0.75 x 1.182 -0.478 -0.01997 0.03413 #\u0026gt; 5 1.00 x 1.833 -0.749 -0.04341 0.07015 #\u0026gt; 6 1.25 x 2.663 -1.104 -0.08330 0.12706 tail(lv_loc_sens) #\u0026gt; x var alpha beta delta gamma #\u0026gt; 397 48.8 y -1.95 -5.45 -0.933 -9.07 #\u0026gt; 398 49.0 y -1.81 -4.95 -0.831 -8.38 #\u0026gt; 399 49.2 y -1.66 -4.43 -0.725 -7.64 #\u0026gt; 400 49.5 y -1.49 -3.90 -0.614 -6.87 #\u0026gt; 401 49.8 y -1.30 -3.34 -0.495 -6.04 #\u0026gt; 402 50.0 y -1.08 -2.75 -0.367 -5.15 summary() can be used to summarize these results over the time series, for example, to see which parameters the model is most sensitive too.\nsummary(lv_loc_sens) #\u0026gt; value scale L1 L2 Mean Min Max N #\u0026gt; alpha 1.0 1.0 6.5 11.2 2.2 -35 44 402 #\u0026gt; beta 0.2 0.2 8.3 12.5 -4.1 -56 33 402 #\u0026gt; delta 0.5 0.5 2.5 4.3 -1.1 -19 11 402 #\u0026gt; gamma 0.2 0.2 10.8 18.2 -0.1 -78 74 402 \\(\\gamma\\) (predator mortality rate) and \\(\\beta\\) (predator search efficiency) have the largest values for the sensitivity function, on average, suggesting that this model is most sensitive to these parameters. There is also plot method for the output of sensFun(), which plots the sensitivity functions as time series.\nplot(lv_loc_sens) However, it’s also possible to use ggplot2 provided I transpose the data to long format first.\nlv_loc_long \u0026lt;- lv_loc_sens %\u0026gt;% gather(parameter, sensitivity, -x, -var) %\u0026gt;% mutate(var = if_else(var == \u0026quot;x\u0026quot;, \u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;)) ggplot(lv_loc_long, aes(x = x, y = sensitivity)) + geom_line(aes(colour = parameter, linetype = var)) + scale_color_brewer(\u0026quot;Parameter\u0026quot;, palette = \u0026quot;Set1\u0026quot;) + scale_linetype_discrete(\u0026quot;Variable\u0026quot;) + labs(title = \u0026quot;Lotka-Volterra parameter sensitivity functions\u0026quot;, subtitle = paste(names(pars), pars, sep = \u0026quot; = \u0026quot;, collapse = \u0026quot;; \u0026quot;), x = \u0026quot;Time\u0026quot;, y = \u0026quot;Sensitivity\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) Clearly this model is particularly sensitive to \\(\\gamma\\). Furthermore, this sensitivity shows peaks every 16 seconds or so, which is the periodicity of the original dynamics. To see what’s going on here, I’ll take a look at what happens to the two species when I increase \\(\\gamma\\) by 1%:\n# original model lv_results \u0026lt;- lv_model(pars, times = seq(0, 50, by = 0.25)) %\u0026gt;% data.frame() %\u0026gt;% gather(var, pop, -time) %\u0026gt;% mutate(var = if_else(var == \u0026quot;x\u0026quot;, \u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;), par = as.character(pars[\u0026quot;gamma\u0026quot;])) # perturbed model new_pars \u0026lt;- pars new_pars[\u0026quot;gamma\u0026quot;] \u0026lt;- new_pars[\u0026quot;gamma\u0026quot;] * 1.1 lv_results_gamma \u0026lt;- lv_model(new_pars, times = seq(0, 50, by = 0.25)) %\u0026gt;% data.frame() %\u0026gt;% gather(var, pop, -time) %\u0026gt;% mutate(var = if_else(var == \u0026quot;x\u0026quot;, \u0026quot;Prey\u0026quot;, \u0026quot;Predator\u0026quot;), par = as.character(new_pars[\u0026quot;gamma\u0026quot;])) # plot ggplot(bind_rows(lv_results, lv_results_gamma), aes(x = time, y = pop)) + geom_line(aes(color = var, linetype = par)) + scale_color_brewer(\u0026quot;Species\u0026quot;, palette = \u0026quot;Set1\u0026quot;) + scale_linetype_discrete(\u0026quot;gamma\u0026quot;) + labs(title = \u0026quot;Lotka-Volterra predator prey model\u0026quot;, subtitle = \u0026quot;Increasing gamma leads to period increasing with time\u0026quot;, x = \u0026quot;Time\u0026quot;, y = \u0026quot;Population density\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) Increasing \\(\\gamma\\) leads to a time dependent period to the dynamics. As a result, the two models initially overlap, but they become increasingly out of sync over time. This explains both the periodicity of the sensitivity function and the increasing amplitude.\n  ","date":1507852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584702671,"objectID":"65ad554cd7c94ddee2b9090ff9037d73","permalink":"https://strimas.com/post/lotka-volterra/","publishdate":"2017-10-13T00:00:00Z","relpermalink":"/post/lotka-volterra/","section":"post","summary":"Analyzing dynamical systems in R. Using the Lotka-Volterra predator prey model as a case-study, I use the R packages deSolve and FME to solve a system of differential equations and perform a sensitivity analysis.","tags":["R","Ecology"],"title":"Lotka-Volterra Predator Prey Model","type":"post"},{"authors":[],"categories":[],"content":" In spatial analysis, we often define grids of points or polygons to sample, index, or partition a study area. For example, we may want to overlay a study area with a grid of points as part of some regular spatial sampling scheme, divide a large region into smaller units for indexing purposes as with UTM grid zones, or slice the study area into subunits over which we summarize a spatial variable. In the latter scenario, the most common approach is to use a raster format, in which a grid of uniform square cells is overlayed on a study area and each cell is assigned a value for the spatial variables of interest. In ecology and conservation applications, variables may include number of individuals of a threatened species per grid cell, elevation, mean annual rainfall, or land use.\nFrom my experience, using square cells is by far the most common method for defining grids; however, other options are possible. In fact, any regular tesselation of the plane (i.e. the tiling of a plane with contiguous regular polygons of the same type), can act as a spatial grid. Tessellation is well studied mathematically and there are just three possible regular tesselations: equilateral triangles, squares, and regular hexagons. A forth option is a diamond pattern arising from merging pairs of equilateral triangles; however diamonds are not regular polygons. The following images from Wikipedia1, 2, 3 demonstrate these tessellations:\n Recently I’ve seen a few instances of the use of hexagonal grids, especially in systematic reserve design, and I’ve become curious about the benefits (and drawbacks) of using them compared to traditional square grids. In this post I’ll discuss the relative benefits and show how to generate different types of grids in R.\nComparing Benefits I begin by comparing the benefits of square and hexagonal grids. Most of these points come directly from this excellent GIS StackExchange question.\nSquare grids Raster datasets are the most ubiquitous type of square grid used in GIS. The most notable benefits of this format compared to hexagonal grids are:\n Simplicity of definition and data storage: the only explicitly geographical information required to define a raster grid are the coordinates of the origin (e.g. bottom left corner), the cell size, and grid dimensions (i.e. number of cells in each direction). The attribute data can be stored as an aspatial matrix, and the geographical location of any cell can be derived given that cell’s position relative to the origin. This makes data storage and retrieval easier since the coordinates of the vertices of each grid cell are not explicitly stored. Ease of resampling to different spatial scales: increasing the spatial resolution of a square grid is just a matter of dividing each grid cell into four. Similarly, decreasing the spatial resolution only requires combining groups of four cells into one, typically with some algebraic operation to aggregate the attribute data to the coarser resolution. Relationship between cells is given: there is no need for computationally expensive spatial operations to determine distances or the adjacency relationship between cells. Combining raster layers is simple: algebraic operations combining multiple raster layers built on the same template simplifies to matrix algebra; no spatial operations are required.   Hexagonal grids Regular hexagons are the closest shape to a circle that can be used for the regular tessellation of a plane and they have additional symmetries compared to squares. These properties give rise to the following benefits.\n Reduced edge effects: a hexagonal grid gives the lowest perimeter to area ratio of any regular tessellation of the plane. In practice, this means that edge effects are minimized when working with hexagonal grids. This is essentially the same reason beehives are built from hexagonal honeycomb: it is the arrangement that minimizes the amount of material used to create a lattice of cells with a given volume. All neighbours are identical: square grids have two classes of neighbours, those in the cardinal directions that share an edge and those in diagonal directions that share a vertex. In contrast, a hexagonal grid cell has six identical neighbouring cells, each sharing one of the six equal length sides. Furthermore, the distance between centroids is the same for all neighbours. Better fit to curved surfaces: when dealing with large areas, where the curvature of the earth becomes important, hexagons are better able to fit this curvature than squares. This is why soccer balls are constructed of hexagonal panels. They look badass: it can’t be denied that hexagonal grids look way more impressive than square grids!    Grids in R Required packages library(dplyr) library(tidyr) library(sp) library(raster) library(rgeos) library(rgbif) library(viridis) library(gridExtra) library(rasterVis) set.seed(1)  Study region In the following demonstrations, I’ll use Sri Lanka as an example study area. The getData() function from the raster package downloads country boundaries from the Global Administrative Areas (GADM) database. I clean this up a little by removing the attribute information and any polygons other than the main island of Sri Lanka.\nstudy_area \u0026lt;- getData(\u0026quot;GADM\u0026quot;, country = \u0026quot;LKA\u0026quot;, level = 0, path = tempdir()) %\u0026gt;% disaggregate %\u0026gt;% geometry study_area \u0026lt;- sapply(study_area@polygons, slot, \u0026quot;area\u0026quot;) %\u0026gt;% {which(. == max(.))} %\u0026gt;% study_area[.] plot(study_area, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = TRUE, cex = 20) text(81.5, 9.5, \u0026quot;Study Area:\\nSri Lanka\u0026quot;)  Creating grids Hexagonal grids There is no function in R that will directly generate a hexagonal grid of polygons covering a given region; however, it can be accomplished by first generating a hexagonal grid of points with spsample, then converting this point grid to a grid of polygons with HexPoints2SpatialPolygons.\nsize \u0026lt;- 0.5 hex_points \u0026lt;- spsample(study_area, type = \u0026quot;hexagonal\u0026quot;, cellsize = size) hex_grid \u0026lt;- HexPoints2SpatialPolygons(hex_points, dx = size) plot(study_area, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = TRUE) plot(hex_points, col = \u0026quot;black\u0026quot;, pch = 20, cex = 0.5, add = T) plot(hex_grid, border = \u0026quot;orange\u0026quot;, add = T) A few issues arise with this simple method:\nspsample generates a different grid of points each time it’s called because the grid offset is chosen randomly by default. This can be fixed by setting the offset parameter explicitly with offset = c(0, 0). Only cells whose centroid is fully within the study area polygon are created. By buffering the study area it’s possible to get full coverage by the grid, which is usually what is desired. In some cases it may be desirable to clip the grid to the study area polygon so that cells on the edge match the shape of the study area. This seems to often be the case when setting up a grid of planning units for systematic reserve design. For example, the official Marxan tutorial takes this approach. Clipping can be performed using rgeos::gIntersection(). The resolution of the grid is determined by the cellsize parameter, which is the distance (\\(d\\)) between centroids of neighbouring cells. Other ways of defining cell size are the area (\\(A\\)), side length (\\(s\\)), or radius (\\(r\\)), and these are all related by:  \\[ A = \\frac{3\\sqrt{3}}{2}s^2=2\\sqrt{3}r^2=\\frac{\\sqrt{3}}{2}d^2 \\]\nI incorporate all these refinements into a function that generates hexagonal grids.\nmake_grid \u0026lt;- function(x, cell_diameter, cell_area, clip = FALSE) { if (missing(cell_diameter)) { if (missing(cell_area)) { stop(\u0026quot;Must provide cell_diameter or cell_area\u0026quot;) } else { cell_diameter \u0026lt;- sqrt(2 * cell_area / sqrt(3)) } } ext \u0026lt;- as(extent(x) + cell_diameter, \u0026quot;SpatialPolygons\u0026quot;) projection(ext) \u0026lt;- projection(x) # generate array of hexagon centers g \u0026lt;- spsample(ext, type = \u0026quot;hexagonal\u0026quot;, cellsize = cell_diameter, offset = c(0.5, 0.5)) # convert center points to hexagons g \u0026lt;- HexPoints2SpatialPolygons(g, dx = cell_diameter) # clip to boundary of study area if (clip) { g \u0026lt;- gIntersection(g, x, byid = TRUE) } else { g \u0026lt;- g[x, ] } # clean up feature IDs row.names(g) \u0026lt;- as.character(1:length(g)) return(g) } Using this function I generate a grid of \\(625km^2\\) (\\(25km\\) by \\(25km\\)) cells with and without clipping. This requires projecting the study area polygon to measure distance in kilometers.\nstudy_area_utm \u0026lt;- CRS(\u0026quot;+proj=utm +zone=44 +datum=WGS84 +units=km +no_defs\u0026quot;) %\u0026gt;% spTransform(study_area, .) # without clipping hex_grid \u0026lt;- make_grid(study_area_utm, cell_area = 625, clip = FALSE) plot(study_area_utm, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = FALSE) plot(hex_grid, border = \u0026quot;orange\u0026quot;, add = TRUE) box() # with clipping hex_grid \u0026lt;- make_grid(study_area_utm, cell_area = 625, clip = TRUE) plot(study_area_utm, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = FALSE) plot(hex_grid, border = \u0026quot;orange\u0026quot;, add = TRUE) box()  Square grid Creating and working with raster datasets in R is well covered elsewhere, for example in the vignettes for the raster package, so I won’t delve too deeply into it. Briefly, RasterLayer objects can easily be created that cover the extent of a Spatial* object. I use a cell size of \\(625km^2\\) to match the above hexagonal grid, and fill the raster with binary data indicating whether cells are inside or outside the study area.\nr \u0026lt;- raster(study_area_utm, resolution = 25) r \u0026lt;- rasterize(study_area_utm, r, field = 1) plot(r, col = \u0026quot;grey50\u0026quot;, axes = FALSE, legend = FALSE, bty=\u0026quot;n\u0026quot;, box=FALSE) plot(study_area_utm, add = TRUE) In addition to the raster formats defined in the raster package, the sp package offers several options for square grids. The class SpatialPixels is used for partial grids (i.e. not every cell included) and stores the coordinates of all included cell centers. SpatialGrid objects store full grids and do not store coordinates explicitly. Underlying both classes is the GridTopology class, which stores the grid template (origin, cell size, and dimensions). I never use these classes since the raster classes and methods are more intuitive and efficient.\nThe final option from the sp package is simply to store a square grid as polygons (SpatialPolygons object), just as I did with the hexagonal grids above. In this case, I find the easiest way to define a grid of square polygons is to start with an empty RasterLayer object, coerce it to SpatialPolygons, and clip it as necessary. I incorporate this into the above grid generation function, which now creates hexagonal or square grids as desired.\nmake_grid \u0026lt;- function(x, type, cell_width, cell_area, clip = FALSE) { if (!type %in% c(\u0026quot;square\u0026quot;, \u0026quot;hexagonal\u0026quot;)) { stop(\u0026quot;Type must be either \u0026#39;square\u0026#39; or \u0026#39;hexagonal\u0026#39;\u0026quot;) } if (missing(cell_width)) { if (missing(cell_area)) { stop(\u0026quot;Must provide cell_width or cell_area\u0026quot;) } else { if (type == \u0026quot;square\u0026quot;) { cell_width \u0026lt;- sqrt(cell_area) } else if (type == \u0026quot;hexagonal\u0026quot;) { cell_width \u0026lt;- sqrt(2 * cell_area / sqrt(3)) } } } # buffered extent of study area to define cells over ext \u0026lt;- as(extent(x) + cell_width, \u0026quot;SpatialPolygons\u0026quot;) projection(ext) \u0026lt;- projection(x) # generate grid if (type == \u0026quot;square\u0026quot;) { g \u0026lt;- raster(ext, resolution = cell_width) g \u0026lt;- as(g, \u0026quot;SpatialPolygons\u0026quot;) } else if (type == \u0026quot;hexagonal\u0026quot;) { # generate array of hexagon centers g \u0026lt;- spsample(ext, type = \u0026quot;hexagonal\u0026quot;, cellsize = cell_width, offset = c(0, 0)) # convert center points to hexagons g \u0026lt;- HexPoints2SpatialPolygons(g, dx = cell_width) } # clip to boundary of study area if (clip) { g \u0026lt;- gIntersection(g, x, byid = TRUE) } else { g \u0026lt;- g[x, ] } # clean up feature IDs row.names(g) \u0026lt;- as.character(1:length(g)) return(g) } Plotting all four types of grids that make_grid() can generate.\n# hex - without clipping hex_grid \u0026lt;- make_grid(study_area_utm, type = \u0026quot;hexagonal\u0026quot;, cell_area = 625, clip = FALSE) plot(study_area_utm, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = FALSE) plot(hex_grid, border = \u0026quot;orange\u0026quot;, add = TRUE) box() # hex - with clipping hex_grid_c \u0026lt;- make_grid(study_area_utm, type = \u0026quot;hexagonal\u0026quot;, cell_area = 625, clip = TRUE) plot(study_area_utm, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = FALSE) plot(hex_grid_c, border = \u0026quot;orange\u0026quot;, add = TRUE) box() # square - without clipping sq_grid \u0026lt;- make_grid(study_area_utm, type = \u0026quot;square\u0026quot;, cell_area = 625, clip = FALSE) plot(study_area_utm, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = FALSE) plot(sq_grid, border = \u0026quot;orange\u0026quot;, add = TRUE) box() # square - with clipping sq_grid_c \u0026lt;- make_grid(study_area_utm, type = \u0026quot;square\u0026quot;, cell_area = 625, clip = TRUE) plot(study_area_utm, col = \u0026quot;grey50\u0026quot;, bg = \u0026quot;light blue\u0026quot;, axes = FALSE) plot(sq_grid_c, border = \u0026quot;orange\u0026quot;, add = TRUE) box()   Working with grids Once you’ve created a hexagonal grid, you’ll likely want to aggregate some data over the grid cells. Here I demonstrate three common aggregation tasks I often run into: aggregating points, polygons, or rasters. In these examples, I’ll use spatial data for Ecuador.\necuador \u0026lt;- getData(name = \u0026quot;GADM\u0026quot;, country = \u0026quot;ECU\u0026quot;, level = 0, path = tempdir()) %\u0026gt;% disaggregate %\u0026gt;% geometry # exclude gapalapos ecuador \u0026lt;- sapply(ecuador@polygons, slot, \u0026quot;area\u0026quot;) %\u0026gt;% {which(. == max(.))} %\u0026gt;% ecuador[.] # albers equal area for south america ecuador \u0026lt;- spTransform(ecuador, CRS( paste(\u0026quot;+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60\u0026quot;, \u0026quot;+x_0=0 +y_0=0 +ellps=aust_SA +units=km +no_defs\u0026quot;))) hex_ecu \u0026lt;- make_grid(ecuador, type = \u0026quot;hexagonal\u0026quot;, cell_area = 2500, clip = FALSE) Point density It’s often necessary to summarize a set of point features (e.g. species occurrence points), over a grid by calculating the point density, i.e. the number of points within each grid cell. As an example, I’ll look at bird observations in Ecuador from eBird.\neBird is online tool for birders to record their sightings. Each month millions of observations are entered into eBird globally, making it among the largest citizen science projects in history. The Global Biodiversity Information Facility (GBIF) is a repository for biodiversity occurrence records. They store and provide access to hundreds of millions of records from thousands of sources, including eBird. The rOpenSci package rgbif provides a nice interface for importing GBIF records into R.\nI grab a subset of eBird sightings for 4 arbitrarily chosen bird families: tanagers (Trochilidae), hummingbirds (Thraupidae), herons (Ardeidae), and hawks (Accipitridae).\nbird_families \u0026lt;- c(\u0026quot;Trochilidae\u0026quot;, \u0026quot;Thraupidae\u0026quot;, \u0026quot;Ardeidae\u0026quot;, \u0026quot;Accipitridae\u0026quot;) families \u0026lt;- data_frame(family = bird_families) %\u0026gt;% group_by(family) %\u0026gt;% do(name_suggest(q = .$family, rank = \u0026quot;family\u0026quot;)) %\u0026gt;% filter(family == canonicalName) %\u0026gt;% dplyr::select(family, key) gb \u0026lt;- occ_search(taxonKey = families$key, country = \u0026quot;EC\u0026quot;, datasetKey = ebird_key, limit = 3000, return = \u0026quot;data\u0026quot;, fields = c(\u0026quot;family\u0026quot;, \u0026quot;species\u0026quot;, \u0026quot;decimalLatitude\u0026quot;, \u0026quot;decimalLongitude\u0026quot;), hasCoordinate = TRUE, hasGeospatialIssue = FALSE) %\u0026gt;% rbind_all %\u0026gt;% rename(lng = decimalLongitude, lat = decimalLatitude) %\u0026gt;% as.data.frame coordinates(gb) \u0026lt;- ~ lng + lat projection(gb) \u0026lt;- projection(study_area) gb \u0026lt;- spTransform(gb, projection(ecuador)) Now I summarize these sightings over the hexagonal grid to get point density, and plot the data in the form of a heat map.\nfill_missing \u0026lt;- expand.grid(id = row.names(hex_ecu), family = bird_families, stringsAsFactors = FALSE) point_density \u0026lt;- over(hex_ecu, gb, returnList = TRUE) %\u0026gt;% plyr::ldply(.fun = function(x) x, .id = \u0026quot;id\u0026quot;) %\u0026gt;% mutate(id = as.character(id)) %\u0026gt;% count(id, family) %\u0026gt;% left_join(fill_missing, ., by = c(\u0026quot;id\u0026quot;, \u0026quot;family\u0026quot;)) %\u0026gt;% # log transform mutate(n = ifelse(is.na(n), -1, log10(n))) %\u0026gt;% spread(family, n, fill = -1) %\u0026gt;% SpatialPolygonsDataFrame(hex_ecu, .) spplot(point_density, bird_families, main = \u0026quot;Ecuador eBird Sightings by Family\u0026quot;, col.regions = c(\u0026quot;grey20\u0026quot;, viridis(255)), colorkey = list( space = \u0026quot;bottom\u0026quot;, at = c(-0.1, seq(0, log10(1200), length.out = 255)), labels = list( at = c(-0.1, log10(c(1, 5, 25, 75, 250, 1200))), labels = c(0, 1, 5, 25, 75, 250, 1200) ) ), xlim = bbexpand(bbox(point_density)[1, ], 0.04), ylim = bbexpand(bbox(point_density)[2, ], 0.04), par.strip.text = list(col = \u0026quot;white\u0026quot;), par.settings = list( strip.background = list(col = \u0026quot;grey40\u0026quot;)) ) I’ve used spplot() here, and chosen to use a logarithmic scale, which means a big mess of legend parameters. Unfortunately, the data aren’t all that interesting, though I think the maps are pretty!\n Polygon coverage Another common task, is determining the extent to which grid cells are covered by a polygon geometry. This could be in terms of absolute area covered or percent coverage. In the context of systematic reserve design, we may have species ranges as polygons and want to know the amount of each grid cell that is suitable habitat for each species. This can help highlight which cells are of highest conservation value.\nAs a simple toy example, I use the boundary of Pastaza State.\npastaza \u0026lt;- getData(name = \u0026quot;GADM\u0026quot;, country = \u0026quot;ECU\u0026quot;, level = 1, path = tempdir()) %\u0026gt;% subset(NAME_1 == \u0026quot;Pastaza\u0026quot;) %\u0026gt;% spTransform(projection(hex_ecu)) And calculate the degree to which it covers the grid cells. To intersect the polygons I use gIntersection(byid = TRUE) from the rgoes package. Note that with byid = TRUE, the intersection is performed at the level of individual features within the geometry. For each resulting intersection polygon the feature ID is composed of the two source polygon IDs separated by a space. gArea(byid = TRUE), also from rgeos, returns the area for each polygon.\n# cell areas hex_area \u0026lt;- make_grid(ecuador, type = \u0026quot;hexagonal\u0026quot;, cell_area = 2500, clip = TRUE) hex_area \u0026lt;- gArea(hex_area, byid = T) %\u0026gt;% data.frame(id = names(.), area = ., stringsAsFactors = FALSE) %\u0026gt;% SpatialPolygonsDataFrame(hex_area, .) hex_cover \u0026lt;- gIntersection(hex_area, pastaza, byid = TRUE) %\u0026gt;% gArea(byid = TRUE) %\u0026gt;% data.frame(id_both = names(.), cover_area = ., stringsAsFactors = FALSE) %\u0026gt;% separate(id_both, \u0026quot;id\u0026quot;, extra = \u0026quot;drop\u0026quot;) %\u0026gt;% merge(hex_area, ., by = \u0026quot;id\u0026quot;) hex_cover$cover_area[is.na(hex_cover$cover_area)] \u0026lt;- 0 hex_cover$pct_cover \u0026lt;- 100 * hex_cover$cover_area / hex_cover$area Finally, I plot these two cover variables, again using spplot().\n# area p1 \u0026lt;- spplot(hex_cover, \u0026quot;cover_area\u0026quot;, col = \u0026quot;white\u0026quot;, lwd = 0.5, main = expression(km^2), col.regions = plasma(256), par.settings = list(axis.line = list(col = \u0026#39;transparent\u0026#39;)), colorkey = list( space = \u0026quot;bottom\u0026quot;, at = seq(0, 2500, length.out = 256), axis.line = list(col = \u0026#39;black\u0026#39;)) ) # percent cover p2 \u0026lt;- spplot(hex_cover, \u0026quot;pct_cover\u0026quot;, col = \u0026quot;white\u0026quot;, lwd = 0.5, main = expression(\u0026quot;%\u0026quot;), col.regions = plasma(256), par.settings = list(axis.line = list(col = \u0026#39;transparent\u0026#39;)), colorkey = list( space = \u0026quot;bottom\u0026quot;, at = seq(0, 100, length.out = 256), axis.line = list(col = \u0026#39;black\u0026#39;)) ) grid.arrange(p1, p2, ncol = 2, top = \u0026quot;Ecuador: Coverage by Pastaza State\u0026quot;) Again, these data are not very interesting, but the example is illustrative.\n Raster aggregation One of the most common operation I find myself doing with hexagonal grids is aggregating raster layers over the grid cells. For example, elevation or climate variables in raster format might be averaged over hexagonal grid cells, then used to parameterize a species distribution model.\nThe getData() function from the raster package provides access to elevation and climate raster datasets. As an example, I’ll use the SRTM elevation dataset, which has been aggregated to 1km resolution. First, I download, crop, and reproject this dataset.\nsrtm \u0026lt;- getData(\u0026#39;alt\u0026#39;, country = \u0026#39;ECU\u0026#39;, path = tempdir()) %\u0026gt;% projectRaster(t_crop, to = raster(hex_ecu, res=1)) %\u0026gt;% setNames(\u0026#39;elevation\u0026#39;) To average this raster dataset over the hexagonal cells, I use the extract() function from the raster package. By default this function returns values from all raster cells that intersect with a given input geometry; however, with parameters fun = mean and sp = TRUE it will average the raster over each polygon and return a SpatialPolygonsDataFrame object with this information.\nhex_srtm \u0026lt;- extract(srtm, hex_ecu, fun = mean, na.rm = TRUE, sp = TRUE) p1 \u0026lt;- levelplot(srtm, col.regions = terrain.colors, margin = FALSE, scales = list(draw = FALSE), colorkey = list( #space = \u0026quot;bottom\u0026quot;, at = seq(0, 6000, length.out = 256), labels = list(at = 1000 * 0:6, labels = format(1000 * 0:6, big.mark = \u0026quot;,\u0026quot;)) ) ) p2 \u0026lt;- spplot(hex_srtm, col.regions = terrain.colors(256), at = seq(0, 4000, length.out = 256), colorkey = list( labels = list(at = seq(0, 4000, 500), labels = format(seq(0, 4000, 500), big.mark = \u0026quot;,\u0026quot;)) ) ) grid.arrange(p1, p2, ncol = 2, top = \u0026quot;Ecuador SRTM Elevation (m)\u0026quot;) The raster package makes this aggregation task extremely easy, just a single line of code! I’ve also used levelplot() from rasterVis, which provides a nice system for mapping raster data.\n Footnotes  1 “1-uniform n11” by Tomruen - Own work. Licensed under CC BY-SA 4.0 via Commons - https://commons.wikimedia.org/wiki/File:1-uniform_n11.svg ↩︎  2 “1-uniform n5” by Tomruen - Own work. Licensed under CC BY-SA 4.0 via Commons - https://commons.wikimedia.org/wiki/File:1-uniform_n5.svg ↩︎  3 “1-uniform n1” by Tomruen - Own work. Licensed under CC BY-SA 4.0 via Commons - https://commons.wikimedia.org/wiki/File:1-uniform_n1.svg ↩︎     ","date":1452729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584706410,"objectID":"847b72d25a5f4c88ada3d104f080c07c","permalink":"https://strimas.com/post/hexagonal-grids/","publishdate":"2016-01-14T00:00:00Z","relpermalink":"/post/hexagonal-grids/","section":"post","summary":"Exploring the benefits of hexagonal grids relative to square grids for spatial sampling and analysis, and generating hexagonal grids in R.","tags":["R","Spatial"],"title":"Fishnets and Honeycomb: Square vs. Hexagonal Spatial Grids","type":"post"},{"authors":["Matt Strimas-Mackey","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://strimas.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Matt Strimas-Mackey","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://strimas.com/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]